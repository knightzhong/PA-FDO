import torch
import torch.nn.functional as F

class ConditionalFlowMatching:
    def __init__(self, model, device):
        self.model = model.to(device)
        self.device = device
        
    def compute_divergence(self, x_in, t, y_target, y_start, drop_mask=None):
        """
        Âà©Áî® Hutchinson ‰º∞ËÆ°Âô®ËÆ°ÁÆóÊï£Â∫¶ div(v)
        Trace(J) approx e^T J e
        """
        x_in = x_in.detach().requires_grad_(True)
        epsilon = torch.randn_like(x_in)
        
        v_pred = self.model(x_in, t, y_target, y_start, drop_mask=drop_mask)
        v_projected = torch.sum(v_pred * epsilon, dim=1)
        
        grad_x = torch.autograd.grad(v_projected.sum(), x_in, create_graph=True)[0]
        div = torch.sum(grad_x * epsilon, dim=1)
        
        return div, v_pred

    def compute_loss(self, x_anchor, x_better, x_worse, y_better, y_worse, y_anchor):
        """
        PA-FDO Ê†∏ÂøÉËÆ≠ÁªÉÈÄªËæë: MSE + Flow-DPO
        """
        x_anchor = x_anchor.to(self.device)
        x_better = x_better.to(self.device)
        x_worse = x_worse.to(self.device)
        y_better = y_better.view(-1, 1).to(self.device)
        y_worse = y_worse.view(-1, 1).to(self.device)
        y_anchor = y_anchor.view(-1, 1).to(self.device)
        
        B = x_anchor.shape[0]
        t = torch.rand(B, 1, device=self.device)
        
        # === 1. Âü∫Á°ÄÊµÅÂåπÈÖç (MSE) ===
        x_t_better = (1 - t) * x_anchor + t * x_better
        u_t = x_better - x_anchor
        drop_mask = (torch.rand(B, 1, device=self.device) < 0.15)
        
        v_pred = self.model(x_t_better, t, y_better, y_anchor, drop_mask=drop_mask)
        loss_mse = torch.mean((v_pred - u_t) ** 2)
        
        # === 2. Ë∑ØÂæÑÂÅèÂ•ΩÊçüÂ§± (Flow-DPO) ===
        # ËÆ°ÁÆó Winner Âíå Loser Ë∑ØÂæÑÂú®Âêå‰∏ÄÁÇπ t ÁöÑÊï£Â∫¶
        x_t_worse = (1 - t) * x_anchor + t * x_worse
        
        # Ê≥®ÊÑèÔºöËÆ°ÁÆó DPO Êó∂ÈÄöÂ∏∏ÂÖ≥Èó≠ drop_mask ‰ª•Ë°°ÈáèÊù°‰ª∂ÊµÅÂú∫
        div_better, _ = self.compute_divergence(x_t_better, t, y_better, y_anchor)
        div_worse, _ = self.compute_divergence(x_t_worse, t, y_worse, y_anchor)
        
        # Loss: ËÆ© Winner Êî∂Áº© (div < 0) ÊØî Loser Êõ¥Âº∫ÔºåÊàñËÄÖ Loser Êâ©Êï£ (div > 0)
        # diff > 0 ÊÑèÂë≥ÁùÄ worse ÁöÑÊï£Â∫¶ÊØî better Â§ß (Á¨¶ÂêàÈ¢ÑÊúü)
        diff = div_worse - div_better
        loss_dpo = -torch.nn.functional.logsigmoid(1.0 * diff).mean()
        
        return loss_mse + 0.1 * loss_dpo

    # @torch.no_grad()
    # def sample(self, x_start, y_target, y_start, 
    #            proxy=None, 
    #            centroid=None,          # Êñ∞Â¢û: ËÆ≠ÁªÉÈõÜË¥®ÂøÉ
    #            steps=100, 
    #            cfg_scale=4.0, 
    #            grad_scale=4.0, 
    #            reg_scale=0.05):        # Êñ∞Â¢û: ÂõûÂ§çÂäõÂº∫Â∫¶
    #     """
    #     Energy-Guided Sampling: CFG + Proxy Gradient + Manifold Restoration
    #     """
    #     self.model.eval()
        
    #     x_current = x_start.clone().to(self.device)
    #     y_target = y_target.to(self.device)
    #     y_start = y_start.to(self.device)
        
    #     # Â¶ÇÊûúÊèê‰æõ‰∫ÜË¥®ÂøÉÔºåÁ°Æ‰øùÂÆÉÂú®ËÆæÂ§á‰∏ä
    #     if centroid is not None:
    #         centroid = centroid.to(self.device)
            
    #     B = x_current.shape[0]
    #     dt = 1.0 / steps
        
    #     mask_uncond = torch.ones((B, 1), device=self.device, dtype=torch.bool)
    #     mask_cond = torch.zeros((B, 1), device=self.device, dtype=torch.bool)
        
    #     if proxy is not None:
    #         proxy.train() # ÂºÄÂêØ Dropout
        
    #     for i in range(steps):
    #         t_val = i / steps
    #         t = torch.full((B, 1), t_val, device=self.device)
            
    #         # === A. ËÆ°ÁÆó‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•Ê¢ØÂ∫¶ (Gradient Guidance) ===
    #         grad_final = torch.zeros_like(x_current)
            
    #         if proxy is not None and grad_scale > 0:
    #             with torch.enable_grad():
    #                 x_in = x_current.detach().clone().requires_grad_(True)
                    
    #                 # MC Dropout Uncertainty
    #                 mc_preds = []
    #                 for _ in range(5):
    #                     mc_preds.append(proxy(x_in))
    #                 mc_preds = torch.stack(mc_preds)
                    
    #                 avg_score = mc_preds.mean(dim=0)
    #                 uncertainty = mc_preds.std(dim=0)
                    
    #                 grad = torch.autograd.grad(avg_score.sum(), x_in)[0]
                    
    #                 # Âä®ÊÄÅÈòªÂ∞º: ‰∏çÁ°ÆÂÆöÊÄßÈ´òÊàñÊé•ËøëÁªàÁÇπÊó∂ÂáèÂ∞èÊ¢ØÂ∫¶Âπ≤Êâ∞
    #                 damping = torch.exp(-2.0 * uncertainty) * (1.0 - t_val)
    #                 grad_final = grad * damping * grad_scale
            
    #         # === B. ËÆ°ÁÆóÂõûÂ§çÂäõ (Manifold Restoration) ===
    #         # Èò≤Ê≠¢Ê†∑Êú¨È£ûÂá∫Êï∞ÊçÆÊµÅÂΩ¢Â§™Ëøú (OOD)
    #         # Force = - k * (x - centroid)
    #         v_reg = torch.zeros_like(x_current)
    #         if centroid is not None and reg_scale > 0:
    #             # ÂÅáËÆæÊï∞ÊçÆÂ∑≤Ê†áÂáÜÂåñÔºåcentroid ÈÄöÂ∏∏Êé•Ëøë 0
    #             # ËøôÁßçÊãâÂäõÈöèË∑ùÁ¶ªÁ∫øÊÄßÂ¢ûÂä†
    #             dist_vec = x_current - centroid
    #             v_reg = - dist_vec * reg_scale
                
    #             # ÂèØÈÄâÔºöÈöèÊó∂Èó¥ t Â¢ûÂä†ÊãâÂäõÔºåÁ°Æ‰øùÁªàÁÇπ‰∏çÂèëÊï£Ôºü
    #             # ÊàñËÄÖÈöèÊó∂Èó¥ t ÂáèÂ∞èÊãâÂäõÔºåÂÖÅËÆ∏Êé¢Á¥¢Ôºü
    #             # ËøôÈáåÊàë‰ª¨‰øùÊåÅÊÅíÂÆöÔºå‰Ωú‰∏∫ËÉåÊôØÂú∫
            
    #         # === C. ËÆ°ÁÆóÊµÅÈÄüÂ∫¶ (CFG) ===
    #         with torch.no_grad():
    #             v_cond = self.model(x_current, t, y_target, y_start, drop_mask=mask_cond)
    #             v_uncond = self.model(x_current, t, y_target, y_start, drop_mask=mask_uncond)
                
    #             v_flow = v_uncond + cfg_scale * (v_cond - v_uncond)
            
    #         # === D. Ê¨ßÊãâÁßØÂàÜ ===
    #         # Total Velocity = Flow + Gradient + Restoration
    #         v_total = v_flow + grad_final + v_reg
    #         x_current = x_current + v_total * dt
            
    #     return x_current
    @torch.no_grad()
    def sample(self, x_start, y_target, y_start, 
               proxy=None, 
               centroid=None, 
               steps=100, 
               cfg_scale=4.0, 
               grad_scale=4.0, 
               reg_scale=0.05):
        """
        Debug Mode Sampling: ÈÄêÂ±ÇÊ£ÄÊü• NaN Êù•Ê∫ê
        """
        self.model.eval()
        
        x_current = x_start.clone().to(self.device)
        y_target = y_target.to(self.device)
        y_start = y_start.to(self.device)
        
        if centroid is not None:
            centroid = centroid.to(self.device)
            
        B = x_current.shape[0]
        dt = 1.0 / steps
        
        mask_uncond = torch.ones((B, 1), device=self.device, dtype=torch.bool)
        mask_cond = torch.zeros((B, 1), device=self.device, dtype=torch.bool)
        
        if proxy is not None:
            proxy.train() # ÂºÄÂêØ Dropout
        
        print(f"\n[Debug] Start Sampling. X Range: [{x_current.min():.2f}, {x_current.max():.2f}]")
        
        # üö®„ÄêËØäÊñ≠ÊèíÊ°©„Äëüö®ÔºöÁúãÁúãÊé®ÁêÜÊó∂ÁöÑÊù°‰ª∂ Y Âà∞Â∫ïÊòØÂ§öÂ∞ë
        print(f"[Debug] Y_Target Stats: Mean={y_target.mean().item():.4f} | Min={y_target.min().item():.4f} | Max={y_target.max().item():.4f}")
        print(f"[Debug] Y_Start  Stats: Mean={y_start.mean().item():.4f}  | Min={y_start.min().item():.4f} | Max={y_start.max().item():.4f}")
        for i in range(steps):
            t_val = i / steps
            t = torch.full((B, 1), t_val, device=self.device)
            
            # --- Check 1: Input Integrity ---
            if torch.isnan(x_current).any():
                print(f"!!! [Step {i}] NaN detected in x_current BEFORE update!")
                break

            # === A. ËÆ°ÁÆó‰∏çÁ°ÆÂÆöÊÄßÊÑüÁü•Ê¢ØÂ∫¶ (Gradient Guidance) ===
            grad_final = torch.zeros_like(x_current)
            
            if proxy is not None and grad_scale > 0:
                with torch.enable_grad():
                    x_in = x_current.detach().clone().requires_grad_(True)
                    
                    # MC Dropout Uncertainty
                    mc_preds = []
                    for _ in range(5):
                        mc_preds.append(proxy(x_in))
                    mc_preds = torch.stack(mc_preds)
                    
                    avg_score = mc_preds.mean(dim=0)
                    uncertainty = mc_preds.std(dim=0)
                    
                    # --- Debug Proxy Output ---
                    if torch.isnan(avg_score).any():
                        print(f"!!! [Step {i}] NaN detected in Proxy Output (avg_score)!")
                        print(f"    Input to proxy Range: [{x_in.min():.2f}, {x_in.max():.2f}]")
                        break
                    
                    grad = torch.autograd.grad(avg_score.sum(), x_in)[0]
                    
                    # --- Debug Raw Gradient ---
                    if torch.isnan(grad).any():
                        print(f"!!! [Step {i}] NaN detected in Raw Gradient!")
                        break
                    
                    # Âä®ÊÄÅÈòªÂ∞º
                    damping = torch.exp(-2.0 * uncertainty) * (1.0 - t_val)
                    grad_final = grad * damping * grad_scale
                    # === „ÄêÊñ∞Â¢û„ÄëÔºöÂº∫Âà∂Ê¢ØÂ∫¶Ë£ÅÂâ™ (Gradient Clipping) ===
                    # Êó†ËÆ∫ÁÆóÂá∫Êù•Â§öÂ§ßÔºåÂº∫Âà∂ÊääÊ¢ØÂ∫¶ÁöÑÊ®°ÈïøÈôêÂà∂Âú® 1.0 ‰ª•ÂÜÖ
                    # ËøôÊ†∑Â∞±ÁÆóÊ®°ÂûãÁñØ‰∫ÜÔºå‰πü‰∏ç‰ºö‰∏ÄÊ≠•ËøàÂá∫Â§™Èò≥Á≥ª
                    grad_norm = grad_final.view(B, -1).norm(dim=1, keepdim=True)
                    clip_coef = torch.clamp(1.0 / (grad_norm + 1e-6), max=1.0)
                    grad_final = grad_final * clip_coef.view(B, -1).expand_as(grad_final)
            
            # === B. ËÆ°ÁÆóÂõûÂ§çÂäõ (Manifold Restoration) ===
            v_reg = torch.zeros_like(x_current)
            if centroid is not None and reg_scale > 0:
                dist_vec = x_current - centroid
                v_reg = - dist_vec * reg_scale
            
            # === C. ËÆ°ÁÆóÊµÅÈÄüÂ∫¶ (CFG) ===
            with torch.no_grad():
                v_cond = self.model(x_current, t, y_target, y_start, drop_mask=mask_cond)
                v_uncond = self.model(x_current, t, y_target, y_start, drop_mask=mask_uncond)
                
                # --- Debug Flow Model Output ---
                if torch.isnan(v_cond).any() or torch.isnan(v_uncond).any():
                    print(f"!!! [Step {i}] NaN detected in Flow Model Output!")
                    print(f"    v_cond NaN: {torch.isnan(v_cond).any()}, v_uncond NaN: {torch.isnan(v_uncond).any()}")
                    break

                v_flow = v_uncond + cfg_scale * (v_cond - v_uncond)
            
            # === D. Ê¨ßÊãâÁßØÂàÜ ===
            v_total = v_flow + grad_final + v_reg
            
            # --- Debug Velocity Components ---
            if i % 10 == 0: # ÊØè10Ê≠•ÊâìÂç∞‰∏ÄÊ¨°Áä∂ÊÄÅ
                v_flow_norm = v_flow.norm().item()
                v_grad_norm = grad_final.norm().item()
                v_reg_norm = v_reg.norm().item()
                x_max = x_current.abs().max().item()
                print(f"[Step {i}] X_max: {x_max:.2f} | Flow: {v_flow_norm:.2f} | Grad: {v_grad_norm:.2f} | Reg: {v_reg_norm:.2f}")
                
                # Â¶ÇÊûúÊüê‰∏™ÂàÜÈáèÁâπÂà´Â§ßÔºåÊèêÂâçÈ¢ÑË≠¶
                if v_grad_norm > 100 or v_flow_norm > 100:
                    print(f"    -> Warning: Velocity Explosion detected at Step {i}!")
            
            x_current = x_current + v_total * dt
            
        return x_current