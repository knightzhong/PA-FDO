import torch
import torch.nn.functional as F

class ConditionalFlowMatching:
    def __init__(self, model, device):
        self.model = model.to(device)
        self.device = device
        
    def compute_divergence(self, x_in, t, y_target, y_start, drop_mask=None):
        """
        åˆ©ç”¨ Hutchinson ä¼°è®¡å™¨è®¡ç®—æ•£åº¦ div(v)
        Trace(J) approx e^T J e
        """
        x_in = x_in.detach().requires_grad_(True)
        epsilon = torch.randn_like(x_in)
        
        v_pred = self.model(x_in, t, y_target, y_start, drop_mask=drop_mask)
        v_projected = torch.sum(v_pred * epsilon, dim=1)
        
        grad_x = torch.autograd.grad(v_projected.sum(), x_in, create_graph=True)[0]
        div = torch.sum(grad_x * epsilon, dim=1)
        
        return div, v_pred

    def compute_loss(self, x_anchor, x_better, x_worse, y_better, y_worse, y_anchor):
        """
        PA-FDO æ ¸å¿ƒè®­ç»ƒé€»è¾‘: MSE + Flow-DPO
        """
        x_anchor = x_anchor.to(self.device)
        x_better = x_better.to(self.device)
        x_worse = x_worse.to(self.device)
        y_better = y_better.view(-1, 1).to(self.device)
        y_worse = y_worse.view(-1, 1).to(self.device)
        y_anchor = y_anchor.view(-1, 1).to(self.device)
        
        B = x_anchor.shape[0]
        t = torch.rand(B, 1, device=self.device)
        
        # === 1. åŸºç¡€æµåŒ¹é… (MSE) ===
        x_t_better = (1 - t) * x_anchor + t * x_better
        u_t = x_better - x_anchor
        drop_mask = (torch.rand(B, 1, device=self.device) < 0.15)
        
        v_pred = self.model(x_t_better, t, y_better, y_anchor, drop_mask=drop_mask)
        loss_mse = torch.mean((v_pred - u_t) ** 2)
        
        # === 2. è·¯å¾„åå¥½æŸå¤± (Flow-DPO) ===
        # è®¡ç®— Winner å’Œ Loser è·¯å¾„åœ¨åŒä¸€ç‚¹ t çš„æ•£åº¦
        x_t_worse = (1 - t) * x_anchor + t * x_worse
        
        # æ³¨æ„ï¼šè®¡ç®— DPO æ—¶é€šå¸¸å…³é—­ drop_mask ä»¥è¡¡é‡æ¡ä»¶æµåœº
        div_better, _ = self.compute_divergence(x_t_better, t, y_better, y_anchor)
        div_worse, _ = self.compute_divergence(x_t_worse, t, y_worse, y_anchor)
        
        # Loss: è®© Winner æ”¶ç¼© (div < 0) æ¯” Loser æ›´å¼ºï¼Œæˆ–è€… Loser æ‰©æ•£ (div > 0)
        # diff > 0 æ„å‘³ç€ worse çš„æ•£åº¦æ¯” better å¤§ (ç¬¦åˆé¢„æœŸ)
        diff = div_worse - div_better
        loss_dpo = -torch.nn.functional.logsigmoid(1.0 * diff).mean()
        # ğŸš¨ çœ‹çœ‹è®­ç»ƒæ—¶çš„é€Ÿåº¦åˆ°åº•æ˜¯å¤šå°‘
        if torch.rand(1).item() < 0.01: # å¶å°”æ‰“å°
            print(f"[Train Debug] v_pred norm: {v_pred.norm(dim=1).mean().item():.2f} | Target u_t norm: {(x_better - x_anchor).norm(dim=1).mean().item():.2f}")
        return loss_mse + 0.1 * loss_dpo

    # @torch.no_grad()
    # def sample(self, x_start, y_target, y_start, 
    #            proxy=None, 
    #            centroid=None,          # æ–°å¢: è®­ç»ƒé›†è´¨å¿ƒ
    #            steps=100, 
    #            cfg_scale=4.0, 
    #            grad_scale=4.0, 
    #            reg_scale=0.05):        # æ–°å¢: å›å¤åŠ›å¼ºåº¦
    #     """
    #     Energy-Guided Sampling: CFG + Proxy Gradient + Manifold Restoration
    #     """
    #     self.model.eval()
        
    #     x_current = x_start.clone().to(self.device)
    #     y_target = y_target.to(self.device)
    #     y_start = y_start.to(self.device)
        
    #     # å¦‚æœæä¾›äº†è´¨å¿ƒï¼Œç¡®ä¿å®ƒåœ¨è®¾å¤‡ä¸Š
    #     if centroid is not None:
    #         centroid = centroid.to(self.device)
            
    #     B = x_current.shape[0]
    #     dt = 1.0 / steps
        
    #     mask_uncond = torch.ones((B, 1), device=self.device, dtype=torch.bool)
    #     mask_cond = torch.zeros((B, 1), device=self.device, dtype=torch.bool)
        
    #     if proxy is not None:
    #         proxy.train() # å¼€å¯ Dropout
        
    #     for i in range(steps):
    #         t_val = i / steps
    #         t = torch.full((B, 1), t_val, device=self.device)
            
    #         # === A. è®¡ç®—ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¢¯åº¦ (Gradient Guidance) ===
    #         grad_final = torch.zeros_like(x_current)
            
    #         if proxy is not None and grad_scale > 0:
    #             with torch.enable_grad():
    #                 x_in = x_current.detach().clone().requires_grad_(True)
                    
    #                 # MC Dropout Uncertainty
    #                 mc_preds = []
    #                 for _ in range(5):
    #                     mc_preds.append(proxy(x_in))
    #                 mc_preds = torch.stack(mc_preds)
                    
    #                 avg_score = mc_preds.mean(dim=0)
    #                 uncertainty = mc_preds.std(dim=0)
                    
    #                 grad = torch.autograd.grad(avg_score.sum(), x_in)[0]
                    
    #                 # åŠ¨æ€é˜»å°¼: ä¸ç¡®å®šæ€§é«˜æˆ–æ¥è¿‘ç»ˆç‚¹æ—¶å‡å°æ¢¯åº¦å¹²æ‰°
    #                 damping = torch.exp(-2.0 * uncertainty) * (1.0 - t_val)
    #                 grad_final = grad * damping * grad_scale
            
    #         # === B. è®¡ç®—å›å¤åŠ› (Manifold Restoration) ===
    #         # é˜²æ­¢æ ·æœ¬é£å‡ºæ•°æ®æµå½¢å¤ªè¿œ (OOD)
    #         # Force = - k * (x - centroid)
    #         v_reg = torch.zeros_like(x_current)
    #         if centroid is not None and reg_scale > 0:
    #             # å‡è®¾æ•°æ®å·²æ ‡å‡†åŒ–ï¼Œcentroid é€šå¸¸æ¥è¿‘ 0
    #             # è¿™ç§æ‹‰åŠ›éšè·ç¦»çº¿æ€§å¢åŠ 
    #             dist_vec = x_current - centroid
    #             v_reg = - dist_vec * reg_scale
                
    #             # å¯é€‰ï¼šéšæ—¶é—´ t å¢åŠ æ‹‰åŠ›ï¼Œç¡®ä¿ç»ˆç‚¹ä¸å‘æ•£ï¼Ÿ
    #             # æˆ–è€…éšæ—¶é—´ t å‡å°æ‹‰åŠ›ï¼Œå…è®¸æ¢ç´¢ï¼Ÿ
    #             # è¿™é‡Œæˆ‘ä»¬ä¿æŒæ’å®šï¼Œä½œä¸ºèƒŒæ™¯åœº
            
    #         # === C. è®¡ç®—æµé€Ÿåº¦ (CFG) ===
    #         with torch.no_grad():
    #             v_cond = self.model(x_current, t, y_target, y_start, drop_mask=mask_cond)
    #             v_uncond = self.model(x_current, t, y_target, y_start, drop_mask=mask_uncond)
                
    #             v_flow = v_uncond + cfg_scale * (v_cond - v_uncond)
            
    #         # === D. æ¬§æ‹‰ç§¯åˆ† ===
    #         # Total Velocity = Flow + Gradient + Restoration
    #         v_total = v_flow + grad_final + v_reg
    #         x_current = x_current + v_total * dt
            
    #     return x_current
    @torch.no_grad()
    def sample(self, x_start, y_target, y_start, 
               proxy=None, 
               centroid=None, 
               steps=100, 
               cfg_scale=4.0, 
               grad_scale=4.0, 
               reg_scale=0.05):
        """
        Debug Mode Sampling: é€å±‚æ£€æŸ¥ NaN æ¥æº
        """
        self.model.eval()
        
        x_current = x_start.clone().to(self.device)
        y_target = y_target.to(self.device)
        y_start = y_start.to(self.device)
        
        if centroid is not None:
            centroid = centroid.to(self.device)
            
        B = x_current.shape[0]
        dt = 1.0 / steps
        
        mask_uncond = torch.ones((B, 1), device=self.device, dtype=torch.bool)
        mask_cond = torch.zeros((B, 1), device=self.device, dtype=torch.bool)
        
        if proxy is not None:
            proxy.train() # å¼€å¯ Dropout
        
        print(f"\n[Debug] Start Sampling. X Range: [{x_current.min():.2f}, {x_current.max():.2f}]")
        
        # ğŸš¨ã€è¯Šæ–­æ’æ¡©ã€‘ğŸš¨ï¼šçœ‹çœ‹æ¨ç†æ—¶çš„æ¡ä»¶ Y åˆ°åº•æ˜¯å¤šå°‘
        print(f"[Debug] Y_Target Stats: Mean={y_target.mean().item():.4f} | Min={y_target.min().item():.4f} | Max={y_target.max().item():.4f}")
        print(f"[Debug] Y_Start  Stats: Mean={y_start.mean().item():.4f}  | Min={y_start.min().item():.4f} | Max={y_start.max().item():.4f}")
        for i in range(steps):
            t_val = i / steps
            t = torch.full((B, 1), t_val, device=self.device)
            
            # --- Check 1: Input Integrity ---
            if torch.isnan(x_current).any():
                print(f"!!! [Step {i}] NaN detected in x_current BEFORE update!")
                break

            # === A. è®¡ç®—ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¢¯åº¦ (Gradient Guidance) ===
            grad_final = torch.zeros_like(x_current)
            
            if proxy is not None and grad_scale > 0:
                with torch.enable_grad():
                    x_in = x_current.detach().clone().requires_grad_(True)
                    
                    # MC Dropout Uncertainty
                    mc_preds = []
                    for _ in range(5):
                        mc_preds.append(proxy(x_in))
                    mc_preds = torch.stack(mc_preds)
                    
                    avg_score = mc_preds.mean(dim=0)
                    uncertainty = mc_preds.std(dim=0)
                    
                    # --- Debug Proxy Output ---
                    if torch.isnan(avg_score).any():
                        print(f"!!! [Step {i}] NaN detected in Proxy Output (avg_score)!")
                        print(f"    Input to proxy Range: [{x_in.min():.2f}, {x_in.max():.2f}]")
                        break
                    
                    grad = torch.autograd.grad(avg_score.sum(), x_in)[0]
                    
                    # --- Debug Raw Gradient ---
                    if torch.isnan(grad).any():
                        print(f"!!! [Step {i}] NaN detected in Raw Gradient!")
                        break
                    
                    # åŠ¨æ€é˜»å°¼
                    damping = torch.exp(-2.0 * uncertainty) * (1.0 - t_val)
                    grad_final = grad * damping * grad_scale
                    # === ã€æ–°å¢ã€‘ï¼šå¼ºåˆ¶æ¢¯åº¦è£å‰ª (Gradient Clipping) ===
                    # æ— è®ºç®—å‡ºæ¥å¤šå¤§ï¼Œå¼ºåˆ¶æŠŠæ¢¯åº¦çš„æ¨¡é•¿é™åˆ¶åœ¨ 1.0 ä»¥å†…
                    # è¿™æ ·å°±ç®—æ¨¡å‹ç–¯äº†ï¼Œä¹Ÿä¸ä¼šä¸€æ­¥è¿ˆå‡ºå¤ªé˜³ç³»
                    grad_norm = grad_final.view(B, -1).norm(dim=1, keepdim=True)
                    clip_coef = torch.clamp(1.0 / (grad_norm + 1e-6), max=1.0)
                    grad_final = grad_final * clip_coef.view(B, -1).expand_as(grad_final)
            
            # === B. è®¡ç®—å›å¤åŠ› (Manifold Restoration) ===
            v_reg = torch.zeros_like(x_current)
            if centroid is not None and reg_scale > 0:
                dist_vec = x_current - centroid
                v_reg = - dist_vec * reg_scale
            
            # === C. è®¡ç®—æµé€Ÿåº¦ (CFG) ===
            with torch.no_grad():
                v_cond = self.model(x_current, t, y_target, y_start, drop_mask=mask_cond)
                v_uncond = self.model(x_current, t, y_target, y_start, drop_mask=mask_uncond)
                
                # --- Debug Flow Model Output ---
                if torch.isnan(v_cond).any() or torch.isnan(v_uncond).any():
                    print(f"!!! [Step {i}] NaN detected in Flow Model Output!")
                    print(f"    v_cond NaN: {torch.isnan(v_cond).any()}, v_uncond NaN: {torch.isnan(v_uncond).any()}")
                    break

                v_flow = v_uncond + cfg_scale * (v_cond - v_uncond)
            
            # === D. æ¬§æ‹‰ç§¯åˆ† ===
            v_total = v_flow + grad_final + v_reg
            # ğŸš¨ğŸš¨ğŸš¨ã€å¿…é¡»è¡¥ä¸Šè¿™ä¸€æ®µã€‘å…¨å±€é€Ÿåº¦æˆªæ–­ ğŸš¨ğŸš¨ğŸš¨
            # æ²¡æœ‰è¿™æ®µä»£ç ï¼Œæ¨¡å‹ 100% ä¼šç‚¸ï¼Œå› ä¸ºåˆå§‹æµåœºå¾€å¾€å¾ˆä¸ç¨³å®š
            v_norm = v_total.view(B, -1).norm(dim=1, keepdim=True)
            # å¼ºåˆ¶é™åˆ¶å•æ­¥é€Ÿåº¦ä¸è¶…è¿‡ 2.0 (åœ¨æ ‡å‡†åŒ–ç©ºé—´é‡Œï¼Œè¿™å·²ç»å¾ˆå¿«äº†)
            clip_coef = torch.clamp(2.0 / (v_norm + 1e-6), max=1.0)
            v_total = v_total * clip_coef.view(B, -1).expand_as(v_total)
            # --- Debug Velocity Components ---
            if i % 10 == 0: # æ¯10æ­¥æ‰“å°ä¸€æ¬¡çŠ¶æ€
                v_flow_norm = v_flow.norm().item()
                v_grad_norm = grad_final.norm().item()
                v_reg_norm = v_reg.norm().item()
                x_max = x_current.abs().max().item()
                print(f"[Step {i}] X_max: {x_max:.2f} | Flow: {v_flow_norm:.2f} | Grad: {v_grad_norm:.2f} | Reg: {v_reg_norm:.2f}")
                
                # å¦‚æœæŸä¸ªåˆ†é‡ç‰¹åˆ«å¤§ï¼Œæå‰é¢„è­¦
                if v_grad_norm > 100 or v_flow_norm > 100:
                    print(f"    -> Warning: Velocity Explosion detected at Step {i}!")
                # å»ºè®®åŠ ä¸€è¡Œæ‰“å°æˆªæ–­åçš„é€Ÿåº¦ï¼Œç¡®è®¤åˆ¹è½¦ç”Ÿæ•ˆäº†
                print(f"[Step {i}] ... Total(Clipped): {v_total.norm().item():.2f}")
            
            x_current = x_current + v_total * dt
            
        return x_current