import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from configs import Config
from data_loader import build_dynamic_loader # æ³¨æ„è¿™é‡Œå‡½æ•°åå˜äº†
from models import VectorFieldNet, RankProxy
from solver import ConditionalFlowMatching

def seed_everything(seed=42):
    import random
    import os
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def batch_optimal_transport_match(x_src, x_tgt):
    """
    Minibatch Optimal Transport (è¿‘ä¼¼)
    åœ¨å½“å‰ batch å†…é‡æ–°æ’åˆ— x_tgtï¼Œä½¿å¾—å®ƒä¸ x_src çš„æ€»è·ç¦»æœ€å°ã€‚
    ç®€å•å®ç°ï¼šè´ªå¿ƒæœ€è¿‘é‚»æˆ–è€…ç®€å•çš„æ’åºåŒ¹é…ã€‚
    è¿™é‡Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æœ€å¤§åŒ–åŒ¹é… (å¦‚æœæ˜¯é«˜ç»´æ•°æ®ï¼Œä½™å¼¦æ¯”æ¬§æ°æ›´ç¨³)ã€‚
    """
    # å½’ä¸€åŒ–è®¡ç®— Cosine Similarity
    x_src_norm = F.normalize(x_src, p=2, dim=1)
    x_tgt_norm = F.normalize(x_tgt, p=2, dim=1)
    
    # (B, B) ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_matrix = torch.mm(x_src_norm, x_tgt_norm.t())
    
    # ç®€å•çš„è´ªå¿ƒåŒ¹é…: æ¯ä¸ª src æ‰¾æœ€ç›¸ä¼¼çš„ tgt
    # æ³¨æ„ï¼šè¿™å¯èƒ½å¯¼è‡´å¤šä¸ª src æ˜ å°„åˆ°åŒä¸€ä¸ª tgt (å¤šå¯¹ä¸€)
    # ä¸ºäº†ä¿æŒå¤šæ ·æ€§ï¼Œæœ€å¥½æ˜¯ä¸€å¯¹ä¸€ã€‚ä½†ä¸ºäº†è®­ç»ƒæ•ˆç‡ï¼Œargmax è¶³çŸ£ï¼Œ
    # æ„å‘³ç€æˆ‘ä»¬åªå­¦ä¹ â€œæœ€å®¹æ˜“åˆ°è¾¾çš„é‚£ä¸ªé«˜åˆ†ç‚¹â€
    best_indices = torch.argmax(sim_matrix, dim=1)
    
    return x_tgt[best_indices], best_indices

def main():
    cfg = Config()
    
    # 1. åŠ è½½æ•°æ® (åŠ¨æ€æ± æ¨¡å¼)
    # task, ds_all, ds_gold, (mean_x, std_x, mean_y, std_y) = build_dynamic_loader(cfg)
    # 1. åŠ è½½æ•°æ® (åªæ‹¿ dataset_fixed)
    task, ds_fixed, _, (mean_x, std_x, mean_y, std_y) = build_dynamic_loader(cfg) # <--- æ¥å£å˜äº†
    ds_all = ds_fixed
    # Loader (ç›´æ¥ shuffle è¿™ä¸ªåŒ…å«é…å¯¹çš„ dataset)
    loader = torch.utils.data.DataLoader(ds_fixed, batch_size=cfg.BATCH_SIZE, shuffle=True, drop_last=True)
    
    # test
    # print(f"Norm Formula: y_norm = (y_raw - {mean_y.item():.4f}) / {std_y.item():.4f}")
    # target_norm_1 = (1.0 - mean_y) / std_y
    # print(f"To get Raw 1.0, we need Norm: {target_norm_1.item():.4f}")
    # import sys
    # sys.exit(0)
    # ç»Ÿè®¡é‡ä¸Šè®¾å¤‡
    mean_x, std_x = mean_x.to(cfg.DEVICE), std_x.to(cfg.DEVICE)
    mean_y, std_y = mean_y.to(cfg.DEVICE), std_y.to(cfg.DEVICE)
    input_dim = ds_all.tensors[0].shape[1]
    
    # DataLoader (Random Samplers)
    # loader_all = torch.utils.data.DataLoader(ds_all, batch_size=cfg.BATCH_SIZE, shuffle=True, drop_last=True)
    # loader_gold = torch.utils.data.DataLoader(ds_gold, batch_size=cfg.BATCH_SIZE, shuffle=True, drop_last=True)
    
    # ç”¨äºæ— é™å¾ªç¯çš„ iterator
    def cycle(loader):
        while True:
            for batch in loader:
                yield batch

    # iter_all = cycle(loader_all)
    # iter_gold = cycle(loader_gold)
    iter_loader = cycle(loader)
    
    # ==========================================
    # Part A: è®­ç»ƒ ListNet Proxy (ICLR 2025 Strategy)
    # ==========================================
    print("\nTraining RankProxy with ListNet Loss (RaM Strategy)...")
    
    # 1. åˆå§‹åŒ–
    proxy = RankProxy(input_dim=input_dim).to(cfg.DEVICE)
    proxy_opt = torch.optim.AdamW(proxy.parameters(), lr=1e-4, weight_decay=1e-5) # è®ºæ–‡å‚æ•°
    
    def listnet_loss(y_pred, y_true, temp=1.0):
        # ---------------------------------------------------------
        # å…³é”®ä¿®æ”¹ï¼šç»™ y_true é™¤ä»¥ä¸€ä¸ªå°çš„æ¸©åº¦ç³»æ•° (tau)
        # è¿™ä¼šæ‹‰å¤§é«˜åˆ†å’Œä½åˆ†çš„å·®è·ï¼Œè®© Target åˆ†å¸ƒæ›´å°–é”
        # ---------------------------------------------------------
        tau = 0.1  # <--- å»ºè®®å°è¯• 0.1 æˆ– 0.05
        
        # é¢„æµ‹å€¼çš„æ¸©åº¦å¯ä»¥ä¿æŒ 1.0ï¼Œæˆ–è€…ä¹Ÿè®¾ä¸º tauï¼Œé€šå¸¸åªé”åŒ– Target æ•ˆæœå°±å¾ˆå¥½
        pred_temp = 1.0 
        
        # è®¡ç®— Log Softmax (é¢„æµ‹)
        p_y_pred = F.log_softmax(y_pred.t() / pred_temp, dim=1)
        
        # è®¡ç®— Softmax (çœŸå®æ ‡ç­¾)ï¼Œé™¤ä»¥ tau è¿›è¡Œé”åŒ–
        # æ¯”å¦‚ y_true=[2.0, 1.0], tau=0.1 -> [20, 10] -> Softmax å·®è·å·¨å¤§
        p_y_true = F.softmax(y_true.t() / tau, dim=1)
        
        return -torch.sum(p_y_true * p_y_pred)

    # å‡†å¤‡å…¨é‡æ•°æ®
    all_x = ds_all.tensors[0].to(cfg.DEVICE)
    all_y = ds_all.tensors[1].to(cfg.DEVICE).view(-1, 1)
    num_samples = all_x.shape[0]
    
    # 3. Listwise è®­ç»ƒå¾ªç¯
    # è®ºæ–‡å»ºè®® List Length (Batch Size) m=100 æˆ– 1000
    list_size = 512 
    maxepo = 5000
    for epoch in range(maxepo):
        proxy.train()
        proxy_opt.zero_grad()
        
        # === Data Augmentation: éšæœºé‡‡æ ·å½¢æˆ List ===
        # æ¯æ¬¡è¿­ä»£éƒ½é‡æ–°é‡‡æ ·ï¼Œç›¸å½“äºæ— é™çš„æ•°æ®å¢å¼º
        idx = torch.randperm(num_samples)[:list_size]
        x_batch = all_x[idx]
        y_batch = all_y[idx]
        
        # === Forward ===
        y_pred = proxy(x_batch)
        
        # === ListNet Loss ===
        # æ¸©åº¦ temp=1.0 æ˜¯æ ‡å‡†è®¾å®šï¼Œå¦‚æœæƒ³è¦æ›´sharpçš„åˆ†å¸ƒå¯ä»¥è°ƒå°
        loss = listnet_loss(y_pred, y_batch)
        
        loss.backward()
        proxy_opt.step()
        
        if (epoch + 1) % 20 == 0:
            pred_std = y_pred.std().item()
            print(f"RaM-ListNet Epoch {epoch+1}/{maxepo} | Loss: {loss.item():.4f} | Pred Std: {pred_std:.4f}")
            # å¦‚æœ Pred Std ä¸€ç›´å¾ˆå° (< 0.01)ï¼Œè¯´æ˜è¾“å‡ºè¿˜æ²¡æ‹‰å¼€å·®è·


    # Proxy Wrapper
    proxy.eval()
    with torch.no_grad():
        # ç”¨å…¨é‡æ•°æ®æ ¡å‡† mean/std
        all_x_gpu = ds_all.tensors[0].to(cfg.DEVICE)
        all_preds = proxy(all_x_gpu)
        proxy_mu = all_preds.mean().item()
        proxy_std = all_preds.std().item()

    class NormalizedProxy(nn.Module):
        def __init__(self, m, mu, std):
            super().__init__()
            self.model = m
            self.mu = mu
            self.std = std
        def forward(self, x):
            return (self.model(x) - self.mu) / (self.std + 1e-8)
            
    norm_proxy = NormalizedProxy(proxy, proxy_mu, proxy_std)
    
    # ==========================================
    # Part B: è®­ç»ƒ Flow Matching (PA-FDO åŠ¨æ€ç‰ˆ)
    # ==========================================
    print("\nTraining Flow Model (PA-FDO Dynamic)...")
    net = VectorFieldNet(input_dim=input_dim, hidden_dim=cfg.LATENT_DIM)
    cfm = ConditionalFlowMatching(net, cfg.DEVICE)
    optimizer = torch.optim.AdamW(net.parameters(), lr=5e-4, weight_decay=1e-5)#torch.optim.Adam(net.parameters(), lr=cfg.LR)
    
    # è®­ç»ƒæ­¥æ•° (Iterations) è€Œé Epochs
    total_steps = 20000 
    
    for step in range(total_steps):
        net.train()
        optimizer.zero_grad()
        
        # 1. ç›´æ¥è·å–é”å®šçš„é…å¯¹ (4é¡¹)
        # x_anc: èµ·ç‚¹
        # y_anc: èµ·ç‚¹åˆ†æ•°
        # x_better: é”å®šçš„ç»ˆç‚¹ (OTé…å¯¹å¥½çš„)
        # y_better: é”å®šçš„ç»ˆç‚¹åˆ†æ•°
        x_anc, y_anc, x_better, y_better = next(iter_loader)
        
        # ä¸Šè®¾å¤‡
        x_anc = x_anc.to(cfg.DEVICE)
        y_anc = y_anc.view(-1, 1).to(cfg.DEVICE)
        x_better = x_better.to(cfg.DEVICE)
        y_better = y_better.view(-1, 1).to(cfg.DEVICE)
        
        # 4. ç”Ÿæˆè‡ªå¯¹æŠ—è´Ÿæ ·æœ¬ (Self-Generated Worse)
        # åˆ©ç”¨å½“å‰æ¨¡å‹èµ°ä¸€æ­¥ï¼Œçœ‹çœ‹ä¼šå»å“ª
        # å¦‚æœå»çš„åœ°æ–¹åˆ†ä½ï¼Œå®ƒå°±æ˜¯æœ€å¥½çš„ x_worse
        net.eval() # é‡‡æ ·æ—¶ç”¨ eval æ¨¡å¼ (å…³é—­ Dropout)
        with torch.no_grad():
            # è¯•æ¢æ­¥: t=0, æœç€ y_better èµ°
            # ä½¿ç”¨ 1-step Euler é¢„æµ‹
            v_initial = net(x_anc, torch.zeros(x_anc.shape[0], 1, device=cfg.DEVICE), y_better, y_anc)
            x_attempt = x_anc + v_initial * 0.1 # å°æ­¥é•¿è¯•æ¢
            
            # Proxy æ‰“åˆ†
            score_attempt = norm_proxy(x_attempt)
            # åŸå§‹åˆ†
            # score_anc = norm_proxy(x_anc)
            
            # å®šä¹‰ "Worse": å¦‚æœç”Ÿæˆçš„ç‚¹åˆ†æ•°æ²¡æœ‰æ˜¾è‘—æé«˜ï¼Œç”šè‡³é™ä½äº†ï¼Œå°±æŠŠå®ƒå½“è´Ÿæ ·æœ¬
            # æˆ–è€…ç®€å•ç²—æš´ï¼šç›´æ¥æŠŠå°è¯•ç”Ÿæˆçš„ç‚¹å½“ä½œ worseï¼Œè¿«ä½¿æ¨¡å‹å»å¯»æ‰¾æ¯”å½“å‰å°è¯•â€œæ›´å¥½â€çš„è·¯å¾„ï¼ˆDPO é€»è¾‘ï¼‰
            # è¿™é‡Œæˆ‘ä»¬å®šä¹‰ï¼šx_worse å°±æ˜¯ x_attempt (æ¨¡å‹å½“å‰å€¾å‘çš„æ–¹å‘)
            x_worse = x_attempt.detach()
            
            # y_worse çš„æ ‡ç­¾ï¼šç”¨ Proxy é¢„æµ‹åˆ†
            y_worse = score_attempt.detach()

        net.train()
        
        # 5. è®¡ç®— Loss (ä¼ å…¥åŠ¨æ€æ„å»ºçš„ä¸‰å…ƒç»„)
        # compute_loss å†…éƒ¨ä¼šè®¡ç®— x_better å’Œ x_worse çš„æ•£åº¦
        loss = cfm.compute_loss(x_anc, x_better, x_worse, y_better, y_worse, y_anc)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)
        optimizer.step()
        
        if (step + 1) % 500 == 0:
            print(f"Step {step+1}/{total_steps} | Loss: {loss.item():.4f}")
            
    # ==========================================
    # Part C: æ¨ç†ä¸è¯„ä¼° (PA-FDO å¢å¼ºç‰ˆ)
    # ==========================================
    print("\nRunning Evaluation with Energy-based Guidance...")
    
    # 1. å‡†å¤‡ç»Ÿè®¡é‡
    # è®­ç»ƒé›†è´¨å¿ƒ (ç”¨äºæ­£åˆ™åŒ–å›å¤åŠ›)
    # æ³¨æ„ï¼šæˆ‘ä»¬çš„æ•°æ®åŠ è½½å™¨è¿”å›çš„æ•°æ®å·²ç»æ˜¯æ ‡å‡†åŒ–çš„ï¼Œæ‰€ä»¥è´¨å¿ƒåº”è¯¥æ¥è¿‘ 0 å‘é‡
    centroid = torch.zeros(1, input_dim, device=cfg.DEVICE)
    # å¦‚æœæƒ³æ›´ç²¾ç¡®ï¼Œå¯ä»¥ç”¨å½“å‰ batch çš„å‡å€¼ï¼Œæˆ–è€… dataset çš„ç»Ÿè®¡é‡
    # centroid = torch.from_numpy(mean_x).to(cfg.DEVICE) # å¦‚æœåœ¨ dataloader é‡Œæ²¡æœ‰å‡å‡å€¼
    # ä½†åœ¨ data_loader.py é‡Œæˆ‘ä»¬åšäº† x = (x - mean)/stdï¼Œæ‰€ä»¥å‡å€¼å°±æ˜¯ 0
    
    # 2. é‡‡æ ·èµ·ç‚¹ (ä» 50th-90th åˆ†ä½)
    y_flat = ds_all.tensors[1].view(-1)
    q50 = torch.quantile(y_flat, 0.5)
    q90 = torch.quantile(y_flat, 0.9)
    mask_start = (y_flat >= q50) & (y_flat <= q90)
    candidate_indices = torch.where(mask_start)[0]
    
    # éšæœºé€‰ batch
    if len(candidate_indices) > cfg.NUM_SAMPLES:
        perm = torch.randperm(len(candidate_indices))[:cfg.NUM_SAMPLES]
        selected_indices = candidate_indices[perm]
    else:
        selected_indices = candidate_indices
        
    x_starts = ds_all.tensors[0][selected_indices].to(cfg.DEVICE)
    y_starts = ds_all.tensors[1][selected_indices].view(-1, 1).to(cfg.DEVICE)
    
    # 2. æ„é€ ç›®æ ‡ (Target) - ã€å…³é”®ä¿®æ”¹ã€‘
    # æˆ‘ä»¬ä¸ä»…è¦è¶…è¶Š y_maxï¼Œæˆ‘ä»¬è¦å»æ˜Ÿè¾°å¤§æµ·ï¼
    # ä¹‹å‰æ˜¯ y_max (1.5)ï¼Œç°åœ¨æˆ‘ä»¬ç›´æ¥è®¾ä¸º 5.0 (å¯¹åº” Raw ~ 0.7)
    # å¦‚æœ 5.0 èƒ½ç¨³ä½ï¼Œä¸‹æ¬¡å°±è®¾ 8.8 (Raw 1.0)
    base_target = 5.0 
    
    y_targets = torch.full_like(y_starts, base_target)
    
    print(f"[Info] Aggressive Targets: Norm={base_target} (Approx Raw 0.7)")
    # 4. æ‰§è¡Œé‡‡æ ·
    x_final = cfm.sample(
        x_starts, 
        y_target=y_targets, 
        y_start=y_starts,
        proxy=norm_proxy,
        centroid=centroid,   # ä¼ å…¥è´¨å¿ƒ
        steps=cfg.ODE_STEPS,
        # === ğŸš¨ ä¸¥æ ¼æ‰§è¡Œè¿™ç»„å‚æ•° ğŸš¨ ===
        
        # 1. å…³æ‰ç«ç®­åŠ©æ¨ (CFG)
        # æ—¢ç„¶æ¨¡å‹èƒ½ç”Ÿæˆ 0.95ï¼Œä¸éœ€è¦ CFG æ”¾å¤§ï¼Œæ±‚ç¨³ï¼
        cfg_scale=1.0,   
        
        # 2. å¼€å¯å¯¼èˆª (Gradient)
        # ä¹‹å‰ä¸ºäº†æµ‹è¯•å…³äº†ï¼Œç°åœ¨å¿…é¡»å¼€ï¼æœ‰äº†å¯¼èˆªï¼Œæ‰èƒ½æŠŠ 80% çš„ 0.4 å˜æˆ 0.9
        # æ”¾å¿ƒï¼Œæœ‰ Clipping (5.0) ä¿æŠ¤ï¼Œå¼€å¯æ¢¯åº¦ä¹Ÿä¸ä¼šç‚¸
        grad_scale=1.0,  
        
        # 3. ä¿æŒå®‰å…¨ç»³
        reg_scale=0.1
    )
    
    # 5. åæ ‡å‡†åŒ–ä¸è¯„ä¼°
    x_denorm = x_final.cpu() * std_x.cpu() + mean_x.cpu()
    print(x_denorm)
    # Oracle è¯„ä¼°
    if hasattr(task, 'predict'):
        if task.is_discrete:
            # ç¦»æ•£ä»»åŠ¡å¤„ç†é€»è¾‘ (å¦‚ TFBind8)
            vocab_size = 4
            seq_len = input_dim // vocab_size
            x_reshaped = x_denorm.view(x_denorm.shape[0], seq_len, vocab_size)
            x_indices = torch.argmax(x_reshaped, dim=2).cpu().numpy()
            scores = task.predict(x_indices)
        else:
            # è¿ç»­ä»»åŠ¡
            scores = task.predict(x_denorm.numpy())
            
        scores = scores.reshape(-1)
        print(scores)
        
        # å½’ä¸€åŒ–åˆ†æ•° (0-100th)
        task_to_min = {'TFBind8-Exact-v0': 0.0, 'TFBind10-Exact-v0': -1.8585268}
        task_to_max = {'TFBind8-Exact-v0': 1.0, 'TFBind10-Exact-v0': 2.1287067}
        oracle_y_min = task_to_min.get(cfg.TASK_NAME, ds_all.tensors[1].min().item())
        oracle_y_max = task_to_max.get(cfg.TASK_NAME, ds_all.tensors[1].max().item())
        # y_min_val = ds_all.tensors[1].min().item()
        # y_max_val = ds_all.tensors[1].max().item()
        norm_scores = (scores - oracle_y_min) / (oracle_y_max - oracle_y_min)
        
        percentiles = np.percentile(norm_scores, [100, 80, 50])
        
        print("-" * 30)
        print(f"Result (Valid {len(scores)}): Mean {norm_scores.mean():.4f}")
        print(f"Percentiles (100/80/50): {percentiles[0]:.4f} | {percentiles[1]:.4f} | {percentiles[2]:.4f}")
        print("-" * 30)
        return percentiles
    else:
        print("Task does not support prediction.")
        return np.zeros(3)

if __name__ == "__main__":
    # ä¸ºäº†æ¼”ç¤ºï¼Œåªè·‘ä¸€ä¸ª Seed
    seed_everything(42)
    main()