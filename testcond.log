nohup: 忽略输入
Experiment: TFBind8-Exact-v0 | Device: cuda
Loading task: TFBind8-Exact-v0...
Data Processed (One-Hot+Noise). X_dim=32. Y_norm Mean=0.00, Std=1.00
Starting Dynamic All-to-Better Pairing (Total: 32898)...
Pairing Result: Total 32898
  - Evolving (Found better): 32665 (99.3%)
  - Staying (Identity map):  233 (0.7%) -> 关键! 保护精英不崩塌
Model Input Dimension: 32

Training RankProxy with ListNet Loss (RaM Strategy)...
RaM-ListNet Epoch 20/200 | Loss: 6.0071 | Pred Std: 0.6541
RaM-ListNet Epoch 40/200 | Loss: 6.0852 | Pred Std: 0.6401
RaM-ListNet Epoch 60/200 | Loss: 5.9866 | Pred Std: 0.6141
RaM-ListNet Epoch 80/200 | Loss: 5.9641 | Pred Std: 0.6056
RaM-ListNet Epoch 100/200 | Loss: 6.0660 | Pred Std: 0.7290
RaM-ListNet Epoch 120/200 | Loss: 5.9493 | Pred Std: 0.7084
RaM-ListNet Epoch 140/200 | Loss: 5.9812 | Pred Std: 0.6627
RaM-ListNet Epoch 160/200 | Loss: 6.0081 | Pred Std: 0.7232
RaM-ListNet Epoch 180/200 | Loss: 5.9763 | Pred Std: 0.6996
RaM-ListNet Epoch 200/200 | Loss: 6.0545 | Pred Std: 0.7567
RaM-ListNet Epoch 220/200 | Loss: 6.1756 | Pred Std: 0.7639
RaM-ListNet Epoch 240/200 | Loss: 6.0229 | Pred Std: 0.7397
RaM-ListNet Epoch 260/200 | Loss: 6.0039 | Pred Std: 0.7013
RaM-ListNet Epoch 280/200 | Loss: 5.8551 | Pred Std: 0.8521
RaM-ListNet Epoch 300/200 | Loss: 5.9510 | Pred Std: 0.8045
RaM-ListNet Epoch 320/200 | Loss: 6.1149 | Pred Std: 0.7919
RaM-ListNet Epoch 340/200 | Loss: 5.9256 | Pred Std: 0.7096
RaM-ListNet Epoch 360/200 | Loss: 6.1008 | Pred Std: 0.7953
RaM-ListNet Epoch 380/200 | Loss: 5.7796 | Pred Std: 0.8347
RaM-ListNet Epoch 400/200 | Loss: 5.9664 | Pred Std: 0.8173
RaM-ListNet Epoch 420/200 | Loss: 5.8151 | Pred Std: 0.8639
RaM-ListNet Epoch 440/200 | Loss: 5.7723 | Pred Std: 0.7990
RaM-ListNet Epoch 460/200 | Loss: 5.9612 | Pred Std: 0.7098
RaM-ListNet Epoch 480/200 | Loss: 5.9887 | Pred Std: 0.7408
RaM-ListNet Epoch 500/200 | Loss: 5.8698 | Pred Std: 0.9453
RaM-ListNet Epoch 520/200 | Loss: 5.8327 | Pred Std: 0.8918
RaM-ListNet Epoch 540/200 | Loss: 5.9372 | Pred Std: 0.7925
RaM-ListNet Epoch 560/200 | Loss: 5.7245 | Pred Std: 0.8737
RaM-ListNet Epoch 580/200 | Loss: 5.8163 | Pred Std: 0.8341
RaM-ListNet Epoch 600/200 | Loss: 5.8659 | Pred Std: 0.8905
RaM-ListNet Epoch 620/200 | Loss: 5.8092 | Pred Std: 0.8710
RaM-ListNet Epoch 640/200 | Loss: 5.6471 | Pred Std: 0.8053
RaM-ListNet Epoch 660/200 | Loss: 5.7302 | Pred Std: 0.8625
RaM-ListNet Epoch 680/200 | Loss: 5.9270 | Pred Std: 0.8945
RaM-ListNet Epoch 700/200 | Loss: 5.7443 | Pred Std: 0.9157
RaM-ListNet Epoch 720/200 | Loss: 5.8568 | Pred Std: 0.9152
RaM-ListNet Epoch 740/200 | Loss: 5.7361 | Pred Std: 0.8431
RaM-ListNet Epoch 760/200 | Loss: 5.7601 | Pred Std: 0.9942
RaM-ListNet Epoch 780/200 | Loss: 5.7785 | Pred Std: 0.9554
RaM-ListNet Epoch 800/200 | Loss: 5.7187 | Pred Std: 0.8950
RaM-ListNet Epoch 820/200 | Loss: 5.7178 | Pred Std: 0.8826
RaM-ListNet Epoch 840/200 | Loss: 5.8408 | Pred Std: 0.9354
RaM-ListNet Epoch 860/200 | Loss: 5.9494 | Pred Std: 0.9109
RaM-ListNet Epoch 880/200 | Loss: 6.0196 | Pred Std: 0.8461
RaM-ListNet Epoch 900/200 | Loss: 5.8677 | Pred Std: 0.8677
RaM-ListNet Epoch 920/200 | Loss: 5.5801 | Pred Std: 0.9631
RaM-ListNet Epoch 940/200 | Loss: 5.8235 | Pred Std: 0.9556
RaM-ListNet Epoch 960/200 | Loss: 5.9251 | Pred Std: 0.8806
RaM-ListNet Epoch 980/200 | Loss: 5.7567 | Pred Std: 0.8360
RaM-ListNet Epoch 1000/200 | Loss: 5.8289 | Pred Std: 0.8772
RaM-ListNet Epoch 1020/200 | Loss: 5.7831 | Pred Std: 0.9601
RaM-ListNet Epoch 1040/200 | Loss: 6.0032 | Pred Std: 0.8495
RaM-ListNet Epoch 1060/200 | Loss: 5.5535 | Pred Std: 0.8798
RaM-ListNet Epoch 1080/200 | Loss: 5.7282 | Pred Std: 1.0344
RaM-ListNet Epoch 1100/200 | Loss: 5.8468 | Pred Std: 0.8383
RaM-ListNet Epoch 1120/200 | Loss: 5.8403 | Pred Std: 1.0114
RaM-ListNet Epoch 1140/200 | Loss: 5.9221 | Pred Std: 0.9439
RaM-ListNet Epoch 1160/200 | Loss: 5.7272 | Pred Std: 0.9489
RaM-ListNet Epoch 1180/200 | Loss: 5.8962 | Pred Std: 0.9100
RaM-ListNet Epoch 1200/200 | Loss: 5.5753 | Pred Std: 0.9554
RaM-ListNet Epoch 1220/200 | Loss: 5.8354 | Pred Std: 0.9525
RaM-ListNet Epoch 1240/200 | Loss: 5.9127 | Pred Std: 0.9564
RaM-ListNet Epoch 1260/200 | Loss: 5.7161 | Pred Std: 0.9473
RaM-ListNet Epoch 1280/200 | Loss: 5.5871 | Pred Std: 0.9184
RaM-ListNet Epoch 1300/200 | Loss: 5.8414 | Pred Std: 0.9744
RaM-ListNet Epoch 1320/200 | Loss: 5.9430 | Pred Std: 0.9959
RaM-ListNet Epoch 1340/200 | Loss: 5.7381 | Pred Std: 0.8872
RaM-ListNet Epoch 1360/200 | Loss: 5.6346 | Pred Std: 0.9338
RaM-ListNet Epoch 1380/200 | Loss: 5.6868 | Pred Std: 0.9647
RaM-ListNet Epoch 1400/200 | Loss: 5.6822 | Pred Std: 0.9806
RaM-ListNet Epoch 1420/200 | Loss: 5.6523 | Pred Std: 1.0416
RaM-ListNet Epoch 1440/200 | Loss: 5.8792 | Pred Std: 0.9727
RaM-ListNet Epoch 1460/200 | Loss: 5.7205 | Pred Std: 0.9629
RaM-ListNet Epoch 1480/200 | Loss: 5.9738 | Pred Std: 1.0672
RaM-ListNet Epoch 1500/200 | Loss: 5.7344 | Pred Std: 0.9691
RaM-ListNet Epoch 1520/200 | Loss: 5.9049 | Pred Std: 0.9652
RaM-ListNet Epoch 1540/200 | Loss: 5.6562 | Pred Std: 0.9859
RaM-ListNet Epoch 1560/200 | Loss: 5.5923 | Pred Std: 0.9226
RaM-ListNet Epoch 1580/200 | Loss: 5.7335 | Pred Std: 0.9719
RaM-ListNet Epoch 1600/200 | Loss: 5.6837 | Pred Std: 0.9134
RaM-ListNet Epoch 1620/200 | Loss: 5.7397 | Pred Std: 0.9600
RaM-ListNet Epoch 1640/200 | Loss: 5.7631 | Pred Std: 0.9597
RaM-ListNet Epoch 1660/200 | Loss: 5.7331 | Pred Std: 0.9607
RaM-ListNet Epoch 1680/200 | Loss: 5.7755 | Pred Std: 0.9805
RaM-ListNet Epoch 1700/200 | Loss: 5.8406 | Pred Std: 0.9366
RaM-ListNet Epoch 1720/200 | Loss: 5.8054 | Pred Std: 0.9765
RaM-ListNet Epoch 1740/200 | Loss: 5.5446 | Pred Std: 0.9977
RaM-ListNet Epoch 1760/200 | Loss: 5.7382 | Pred Std: 1.0097
RaM-ListNet Epoch 1780/200 | Loss: 5.4552 | Pred Std: 1.0603
RaM-ListNet Epoch 1800/200 | Loss: 5.7266 | Pred Std: 1.0557
RaM-ListNet Epoch 1820/200 | Loss: 5.6460 | Pred Std: 1.0529
RaM-ListNet Epoch 1840/200 | Loss: 5.5962 | Pred Std: 1.0044
RaM-ListNet Epoch 1860/200 | Loss: 5.6520 | Pred Std: 0.9892
RaM-ListNet Epoch 1880/200 | Loss: 5.7314 | Pred Std: 1.0162
RaM-ListNet Epoch 1900/200 | Loss: 5.7087 | Pred Std: 1.0197
RaM-ListNet Epoch 1920/200 | Loss: 5.8597 | Pred Std: 1.0652
RaM-ListNet Epoch 1940/200 | Loss: 5.7059 | Pred Std: 1.0184
RaM-ListNet Epoch 1960/200 | Loss: 5.7203 | Pred Std: 0.9775
RaM-ListNet Epoch 1980/200 | Loss: 5.8648 | Pred Std: 1.0645
RaM-ListNet Epoch 2000/200 | Loss: 5.7983 | Pred Std: 1.0906
Performing Output Adaptation...
Proxy Statistics: Mean=-0.3140, Std=1.0966
These will be used to normalize gradients during sampling.

Start Training Flow Model...
Flow Epoch 10/100 | Loss: 0.411957
Flow Epoch 20/100 | Loss: 0.308911
Flow Epoch 30/100 | Loss: 0.265020
Flow Epoch 40/100 | Loss: 0.237447
Flow Epoch 50/100 | Loss: 0.219899
Flow Epoch 60/100 | Loss: 0.210307
Flow Epoch 70/100 | Loss: 0.203710
Flow Epoch 80/100 | Loss: 0.196218
Flow Epoch 90/100 | Loss: 0.193823
Flow Epoch 100/100 | Loss: 0.188265
DEBUG: Top 5 indices: [17123 11687 28136 32111 15662]
DEBUG: Start Scores Mean: 0.3047 (Should be > 0.9)
normalized_scores: [0.46933368 0.532831   0.9161444  0.599565   0.46320042 0.94110006
 0.35349235 0.7152826  0.47641528 0.567971   0.5127098  0.46893167
 0.5631262  0.5178844  0.49609327 0.23305364 0.4735703  0.6101616
 0.44790336 0.42673072 0.5341092  0.41237167 0.5417474  0.42969942
 0.80251926 0.5121119  0.46589082 0.4195151  0.67267966 0.498093
 0.60080194 0.5341092  0.8741187  0.52329606 0.7277553  0.5662702
 0.57622766 0.56343544 0.37431452 0.44674885 0.23135282 0.15399125
 0.531862   0.5122253  0.42229828 0.64198244 0.43989402 0.43145177
 0.5794232  0.54172677 0.4391828  0.6143673  0.63376695 0.17343216
 0.5189564  0.33310312 0.43986312 0.5257391  0.63309693 0.38229293
 0.433668   0.29876715 0.5638581  0.7394962  0.51992536 0.8854884
 0.291768   0.9355441  0.33892715 0.4192574  0.25367996 0.4622418
 0.601874   0.5888962  0.74855685 0.70923185 0.6655053  0.79189175
 0.6826887  0.35349235 0.5988847  0.601874   0.601874   0.87806666
 0.35349235 0.59208137 0.9355441  0.5587453  0.46933368 0.7183029
 0.6781326  0.4082897  0.6184493  0.5417371  0.79356164 0.76902854
 0.4810642  0.53718096 0.6238919  0.5838247  0.3138581  0.49081558
 0.5642395  0.3995176  0.67964786 0.29046923 0.56948626 0.49609327
 0.601874   0.6781326  0.8741187  0.5497155  0.1510947  0.6184493
 0.2980353  0.48809427 0.41810292 0.5099782  0.40757844 0.7160145
 0.5355214  0.5888962  0.67174166 0.36250153 0.49257824 0.88839525
 0.65079576 0.54620045]
------------------------------
Optimization Results (Proxy Screened Top 128):
Optimized Score Mean (valid only): 0.5431
Optimized Score Max (valid only):  0.9411
Normalized Percentile Scores (100th / 80th / 50th): 0.9411 | 0.6692 | 0.5348
------------------------------
Seed: 0 completed
Experiment: TFBind8-Exact-v0 | Device: cuda
Loading task: TFBind8-Exact-v0...
Data Processed (One-Hot+Noise). X_dim=32. Y_norm Mean=0.00, Std=1.00
Starting Dynamic All-to-Better Pairing (Total: 32898)...
Pairing Result: Total 32898
  - Evolving (Found better): 32661 (99.3%)
  - Staying (Identity map):  237 (0.7%) -> 关键! 保护精英不崩塌
Model Input Dimension: 32

Training RankProxy with ListNet Loss (RaM Strategy)...
RaM-ListNet Epoch 20/200 | Loss: 6.2006 | Pred Std: 0.6489
RaM-ListNet Epoch 40/200 | Loss: 6.0665 | Pred Std: 0.5843
RaM-ListNet Epoch 60/200 | Loss: 6.1263 | Pred Std: 0.6569
RaM-ListNet Epoch 80/200 | Loss: 6.0054 | Pred Std: 0.6022
RaM-ListNet Epoch 100/200 | Loss: 6.1152 | Pred Std: 0.6372
RaM-ListNet Epoch 120/200 | Loss: 6.1530 | Pred Std: 0.7086
RaM-ListNet Epoch 140/200 | Loss: 5.9869 | Pred Std: 0.7293
RaM-ListNet Epoch 160/200 | Loss: 5.8163 | Pred Std: 0.8190
RaM-ListNet Epoch 180/200 | Loss: 5.8462 | Pred Std: 0.7792
RaM-ListNet Epoch 200/200 | Loss: 5.9575 | Pred Std: 0.7014
RaM-ListNet Epoch 220/200 | Loss: 5.9684 | Pred Std: 0.7392
RaM-ListNet Epoch 240/200 | Loss: 6.0115 | Pred Std: 0.8014
RaM-ListNet Epoch 260/200 | Loss: 6.0678 | Pred Std: 0.7196
RaM-ListNet Epoch 280/200 | Loss: 5.9786 | Pred Std: 0.7403
RaM-ListNet Epoch 300/200 | Loss: 5.9819 | Pred Std: 0.8296
RaM-ListNet Epoch 320/200 | Loss: 6.0566 | Pred Std: 0.8006
RaM-ListNet Epoch 340/200 | Loss: 5.9726 | Pred Std: 0.7379
RaM-ListNet Epoch 360/200 | Loss: 5.9264 | Pred Std: 0.8349
RaM-ListNet Epoch 380/200 | Loss: 5.9385 | Pred Std: 0.7922
RaM-ListNet Epoch 400/200 | Loss: 6.0052 | Pred Std: 0.7882
RaM-ListNet Epoch 420/200 | Loss: 6.0558 | Pred Std: 0.7679
RaM-ListNet Epoch 440/200 | Loss: 6.3036 | Pred Std: 0.7431
RaM-ListNet Epoch 460/200 | Loss: 5.9707 | Pred Std: 0.8356
RaM-ListNet Epoch 480/200 | Loss: 5.7832 | Pred Std: 0.8813
RaM-ListNet Epoch 500/200 | Loss: 5.9692 | Pred Std: 0.8506
RaM-ListNet Epoch 520/200 | Loss: 6.1155 | Pred Std: 0.8529
RaM-ListNet Epoch 540/200 | Loss: 5.7546 | Pred Std: 0.8074
RaM-ListNet Epoch 560/200 | Loss: 5.9620 | Pred Std: 0.8179
RaM-ListNet Epoch 580/200 | Loss: 5.9408 | Pred Std: 0.8170
RaM-ListNet Epoch 600/200 | Loss: 5.9452 | Pred Std: 0.8723
RaM-ListNet Epoch 620/200 | Loss: 5.8378 | Pred Std: 0.8707
RaM-ListNet Epoch 640/200 | Loss: 6.0834 | Pred Std: 0.9412
RaM-ListNet Epoch 660/200 | Loss: 5.8696 | Pred Std: 0.8729
RaM-ListNet Epoch 680/200 | Loss: 5.9705 | Pred Std: 0.8526
RaM-ListNet Epoch 700/200 | Loss: 5.6288 | Pred Std: 0.9144
RaM-ListNet Epoch 720/200 | Loss: 5.6613 | Pred Std: 0.9811
RaM-ListNet Epoch 740/200 | Loss: 5.9650 | Pred Std: 0.8961
RaM-ListNet Epoch 760/200 | Loss: 5.7404 | Pred Std: 0.8503
RaM-ListNet Epoch 780/200 | Loss: 5.9221 | Pred Std: 0.8847
RaM-ListNet Epoch 800/200 | Loss: 5.5632 | Pred Std: 0.9779
RaM-ListNet Epoch 820/200 | Loss: 5.9496 | Pred Std: 0.8404
RaM-ListNet Epoch 840/200 | Loss: 5.8156 | Pred Std: 0.9331
RaM-ListNet Epoch 860/200 | Loss: 5.8667 | Pred Std: 0.9085
RaM-ListNet Epoch 880/200 | Loss: 5.9380 | Pred Std: 0.8837
RaM-ListNet Epoch 900/200 | Loss: 6.0085 | Pred Std: 0.9263
RaM-ListNet Epoch 920/200 | Loss: 5.7430 | Pred Std: 0.8927
RaM-ListNet Epoch 940/200 | Loss: 6.0929 | Pred Std: 0.8916
RaM-ListNet Epoch 960/200 | Loss: 5.8254 | Pred Std: 0.8410
RaM-ListNet Epoch 980/200 | Loss: 5.7935 | Pred Std: 1.0285
RaM-ListNet Epoch 1000/200 | Loss: 5.9570 | Pred Std: 0.8873
RaM-ListNet Epoch 1020/200 | Loss: 5.8052 | Pred Std: 0.8786
RaM-ListNet Epoch 1040/200 | Loss: 5.9069 | Pred Std: 0.9681
RaM-ListNet Epoch 1060/200 | Loss: 5.7292 | Pred Std: 0.9261
RaM-ListNet Epoch 1080/200 | Loss: 5.6302 | Pred Std: 0.9844
RaM-ListNet Epoch 1100/200 | Loss: 5.8580 | Pred Std: 0.8386
RaM-ListNet Epoch 1120/200 | Loss: 5.9416 | Pred Std: 0.8693
RaM-ListNet Epoch 1140/200 | Loss: 5.8241 | Pred Std: 0.9309
RaM-ListNet Epoch 1160/200 | Loss: 5.8325 | Pred Std: 0.9135
RaM-ListNet Epoch 1180/200 | Loss: 5.8466 | Pred Std: 0.9692
RaM-ListNet Epoch 1200/200 | Loss: 5.7115 | Pred Std: 0.9580
RaM-ListNet Epoch 1220/200 | Loss: 5.6743 | Pred Std: 0.9556
RaM-ListNet Epoch 1240/200 | Loss: 5.7544 | Pred Std: 0.9453
RaM-ListNet Epoch 1260/200 | Loss: 5.9008 | Pred Std: 0.9529
RaM-ListNet Epoch 1280/200 | Loss: 5.6073 | Pred Std: 0.9570
RaM-ListNet Epoch 1300/200 | Loss: 5.9027 | Pred Std: 0.9176
RaM-ListNet Epoch 1320/200 | Loss: 5.9035 | Pred Std: 0.9134
RaM-ListNet Epoch 1340/200 | Loss: 5.7549 | Pred Std: 0.9661
RaM-ListNet Epoch 1360/200 | Loss: 5.8479 | Pred Std: 0.9009
RaM-ListNet Epoch 1380/200 | Loss: 5.7325 | Pred Std: 0.9891
RaM-ListNet Epoch 1400/200 | Loss: 5.8815 | Pred Std: 0.9514
RaM-ListNet Epoch 1420/200 | Loss: 5.8568 | Pred Std: 0.9852
RaM-ListNet Epoch 1440/200 | Loss: 5.9361 | Pred Std: 1.0160
RaM-ListNet Epoch 1460/200 | Loss: 6.0148 | Pred Std: 0.9999
RaM-ListNet Epoch 1480/200 | Loss: 5.9142 | Pred Std: 0.9373
RaM-ListNet Epoch 1500/200 | Loss: 5.7796 | Pred Std: 0.9947
RaM-ListNet Epoch 1520/200 | Loss: 5.8583 | Pred Std: 0.9296
RaM-ListNet Epoch 1540/200 | Loss: 5.7335 | Pred Std: 0.9296
RaM-ListNet Epoch 1560/200 | Loss: 5.8758 | Pred Std: 1.0527
RaM-ListNet Epoch 1580/200 | Loss: 5.7695 | Pred Std: 0.9161
RaM-ListNet Epoch 1600/200 | Loss: 5.6305 | Pred Std: 0.9574
RaM-ListNet Epoch 1620/200 | Loss: 5.8860 | Pred Std: 0.8839
RaM-ListNet Epoch 1640/200 | Loss: 5.9100 | Pred Std: 1.0016
RaM-ListNet Epoch 1660/200 | Loss: 5.6925 | Pred Std: 0.9378
RaM-ListNet Epoch 1680/200 | Loss: 5.6375 | Pred Std: 0.9813
RaM-ListNet Epoch 1700/200 | Loss: 5.8970 | Pred Std: 0.9365
RaM-ListNet Epoch 1720/200 | Loss: 5.8839 | Pred Std: 0.9303
RaM-ListNet Epoch 1740/200 | Loss: 5.5750 | Pred Std: 0.9658
RaM-ListNet Epoch 1760/200 | Loss: 5.6368 | Pred Std: 1.0990
RaM-ListNet Epoch 1780/200 | Loss: 5.5642 | Pred Std: 0.9972
RaM-ListNet Epoch 1800/200 | Loss: 5.6874 | Pred Std: 0.9754
RaM-ListNet Epoch 1820/200 | Loss: 5.7217 | Pred Std: 0.9842
RaM-ListNet Epoch 1840/200 | Loss: 5.5623 | Pred Std: 1.0400
RaM-ListNet Epoch 1860/200 | Loss: 5.4445 | Pred Std: 1.0221
RaM-ListNet Epoch 1880/200 | Loss: 5.7841 | Pred Std: 0.9725
RaM-ListNet Epoch 1900/200 | Loss: 5.6257 | Pred Std: 1.0411
RaM-ListNet Epoch 1920/200 | Loss: 5.7136 | Pred Std: 1.0464
RaM-ListNet Epoch 1940/200 | Loss: 5.5620 | Pred Std: 1.0378
RaM-ListNet Epoch 1960/200 | Loss: 5.6871 | Pred Std: 1.0480
RaM-ListNet Epoch 1980/200 | Loss: 5.4585 | Pred Std: 0.9889
RaM-ListNet Epoch 2000/200 | Loss: 5.8356 | Pred Std: 1.0534
Performing Output Adaptation...
Proxy Statistics: Mean=-0.9111, Std=1.0902
These will be used to normalize gradients during sampling.

Start Training Flow Model...
Flow Epoch 10/100 | Loss: 0.409477
Flow Epoch 20/100 | Loss: 0.307188
Flow Epoch 30/100 | Loss: 0.262427
Flow Epoch 40/100 | Loss: 0.239752
Flow Epoch 50/100 | Loss: 0.227380
Flow Epoch 60/100 | Loss: 0.216069
Flow Epoch 70/100 | Loss: 0.208678
Flow Epoch 80/100 | Loss: 0.201593
Flow Epoch 90/100 | Loss: 0.197225
Flow Epoch 100/100 | Loss: 0.192746
DEBUG: Top 5 indices: [17123 11687 28136 32111 15662]
DEBUG: Start Scores Mean: 0.2479 (Should be > 0.9)
normalized_scores: [0.41389725 0.35451284 0.60607964 0.45097515 0.7465468  0.5030821
 0.5127098  0.4859708  0.8976209  0.27375996 0.7758834  0.667938
 0.4162784  0.2996227  0.3525234  0.6680823  0.7183132  0.5322125
 0.2854183  0.5597452  0.5008143  0.8163732  0.6120068  0.5127098
 0.95489216 0.19999588 0.48396075 0.48281655 0.87049025 0.7183132
 0.37047994 0.4871253  0.30353978 0.59187526 0.42504022 0.91408277
 0.3308972  0.5077207  0.6149961  0.3308972  0.37100565 0.3275574
 0.91408277 0.4756731  0.37590194 0.45124313 0.33264956 0.6680823
 0.5381808  0.31321898 0.29560262 0.34100935 0.42018512 0.82988703
 0.34796727 0.35759494 0.17213334 0.4749     0.5256051  0.67333937
 0.38478744 0.22187977 0.97972417 0.51992536 0.41175318 0.60067827
 0.22187977 0.70150083 0.5127098  0.3998165  0.45019174 0.5614563
 0.3740053  0.38655013 0.37473714 0.3116934  0.2147157  0.6393127
 0.45758256 0.56303346 0.92548347 0.53587186 0.9248959  0.4311838
 0.34796727 0.40738258 0.52427536 0.59033936 0.40977404 0.3071888
 0.92216426 0.33331957 0.546819   0.54078877 0.20445924 0.77053356
 0.34796727 0.52140975 0.44649115 0.5256051  0.8903847  0.27778006
 0.40515608 0.65079576 0.7573496  0.8066631  0.91408277 0.56306434
 0.41958725 0.2904383  0.4147528  0.41286644 0.6769472  0.45124313
 0.45019174 0.5127098  0.7906857  0.27778006 0.4253185  0.42575145
 0.5179153  0.23677483 0.2147157  0.5202655  0.586948   0.41237167
 0.26888427 0.71090174]
------------------------------
Optimization Results (Proxy Screened Top 128):
Optimized Score Mean (valid only): 0.5070
Optimized Score Max (valid only):  0.9797
Normalized Percentile Scores (100th / 80th / 50th): 0.9797 | 0.6681 | 0.4792
------------------------------
Seed: 1 completed
Experiment: TFBind8-Exact-v0 | Device: cuda
Loading task: TFBind8-Exact-v0...
Data Processed (One-Hot+Noise). X_dim=32. Y_norm Mean=0.00, Std=1.00
Starting Dynamic All-to-Better Pairing (Total: 32898)...
Pairing Result: Total 32898
  - Evolving (Found better): 32667 (99.3%)
  - Staying (Identity map):  231 (0.7%) -> 关键! 保护精英不崩塌
Model Input Dimension: 32

Training RankProxy with ListNet Loss (RaM Strategy)...
RaM-ListNet Epoch 20/200 | Loss: 6.1411 | Pred Std: 0.6574
RaM-ListNet Epoch 40/200 | Loss: 6.0223 | Pred Std: 0.6326
RaM-ListNet Epoch 60/200 | Loss: 6.0220 | Pred Std: 0.7115
RaM-ListNet Epoch 80/200 | Loss: 6.0859 | Pred Std: 0.6472
RaM-ListNet Epoch 100/200 | Loss: 6.1688 | Pred Std: 0.5985
RaM-ListNet Epoch 120/200 | Loss: 6.1203 | Pred Std: 0.6950
RaM-ListNet Epoch 140/200 | Loss: 5.9600 | Pred Std: 0.6938
RaM-ListNet Epoch 160/200 | Loss: 6.0064 | Pred Std: 0.7193
RaM-ListNet Epoch 180/200 | Loss: 6.1007 | Pred Std: 0.8045
RaM-ListNet Epoch 200/200 | Loss: 6.0772 | Pred Std: 0.7123
RaM-ListNet Epoch 220/200 | Loss: 6.1329 | Pred Std: 0.6915
RaM-ListNet Epoch 240/200 | Loss: 5.9112 | Pred Std: 0.7793
RaM-ListNet Epoch 260/200 | Loss: 5.9534 | Pred Std: 0.7971
RaM-ListNet Epoch 280/200 | Loss: 5.9520 | Pred Std: 0.7878
RaM-ListNet Epoch 300/200 | Loss: 6.0350 | Pred Std: 0.8223
RaM-ListNet Epoch 320/200 | Loss: 5.9057 | Pred Std: 0.7832
RaM-ListNet Epoch 340/200 | Loss: 6.0464 | Pred Std: 0.6938
RaM-ListNet Epoch 360/200 | Loss: 5.8456 | Pred Std: 0.7765
RaM-ListNet Epoch 380/200 | Loss: 5.8317 | Pred Std: 0.8660
RaM-ListNet Epoch 400/200 | Loss: 5.9251 | Pred Std: 0.8497
RaM-ListNet Epoch 420/200 | Loss: 5.9699 | Pred Std: 0.8182
RaM-ListNet Epoch 440/200 | Loss: 6.1066 | Pred Std: 0.7999
RaM-ListNet Epoch 460/200 | Loss: 5.8708 | Pred Std: 0.7961
RaM-ListNet Epoch 480/200 | Loss: 5.8057 | Pred Std: 0.8537
RaM-ListNet Epoch 500/200 | Loss: 5.9431 | Pred Std: 0.8367
RaM-ListNet Epoch 520/200 | Loss: 5.9518 | Pred Std: 0.8068
RaM-ListNet Epoch 540/200 | Loss: 6.2433 | Pred Std: 0.8475
RaM-ListNet Epoch 560/200 | Loss: 6.0449 | Pred Std: 0.8637
RaM-ListNet Epoch 580/200 | Loss: 5.7080 | Pred Std: 0.8251
RaM-ListNet Epoch 600/200 | Loss: 5.8240 | Pred Std: 0.8561
RaM-ListNet Epoch 620/200 | Loss: 5.8727 | Pred Std: 0.8701
RaM-ListNet Epoch 640/200 | Loss: 6.0586 | Pred Std: 0.8313
RaM-ListNet Epoch 660/200 | Loss: 6.0194 | Pred Std: 0.8724
RaM-ListNet Epoch 680/200 | Loss: 5.9150 | Pred Std: 0.8544
RaM-ListNet Epoch 700/200 | Loss: 6.0610 | Pred Std: 0.8992
RaM-ListNet Epoch 720/200 | Loss: 6.1066 | Pred Std: 0.8116
RaM-ListNet Epoch 740/200 | Loss: 5.8517 | Pred Std: 0.8923
RaM-ListNet Epoch 760/200 | Loss: 5.8112 | Pred Std: 0.7961
RaM-ListNet Epoch 780/200 | Loss: 5.7471 | Pred Std: 0.9161
RaM-ListNet Epoch 800/200 | Loss: 6.0167 | Pred Std: 0.8452
RaM-ListNet Epoch 820/200 | Loss: 5.8088 | Pred Std: 0.8482
RaM-ListNet Epoch 840/200 | Loss: 5.7714 | Pred Std: 0.8912
RaM-ListNet Epoch 860/200 | Loss: 5.7998 | Pred Std: 0.9486
RaM-ListNet Epoch 880/200 | Loss: 6.1190 | Pred Std: 0.9078
RaM-ListNet Epoch 900/200 | Loss: 5.9286 | Pred Std: 0.8482
RaM-ListNet Epoch 920/200 | Loss: 6.0542 | Pred Std: 0.8549
RaM-ListNet Epoch 940/200 | Loss: 5.6979 | Pred Std: 0.9427
RaM-ListNet Epoch 960/200 | Loss: 5.9745 | Pred Std: 0.8887
RaM-ListNet Epoch 980/200 | Loss: 5.6967 | Pred Std: 0.8376
RaM-ListNet Epoch 1000/200 | Loss: 5.7267 | Pred Std: 0.8759
RaM-ListNet Epoch 1020/200 | Loss: 5.9138 | Pred Std: 0.9050
RaM-ListNet Epoch 1040/200 | Loss: 5.8095 | Pred Std: 0.9302
RaM-ListNet Epoch 1060/200 | Loss: 5.5741 | Pred Std: 0.8476
RaM-ListNet Epoch 1080/200 | Loss: 5.9379 | Pred Std: 0.9364
RaM-ListNet Epoch 1100/200 | Loss: 5.9608 | Pred Std: 0.9299
RaM-ListNet Epoch 1120/200 | Loss: 5.8835 | Pred Std: 0.9794
RaM-ListNet Epoch 1140/200 | Loss: 5.7164 | Pred Std: 0.9796
RaM-ListNet Epoch 1160/200 | Loss: 5.8327 | Pred Std: 0.9598
RaM-ListNet Epoch 1180/200 | Loss: 5.8804 | Pred Std: 0.9570
RaM-ListNet Epoch 1200/200 | Loss: 5.7668 | Pred Std: 0.9273
RaM-ListNet Epoch 1220/200 | Loss: 5.6217 | Pred Std: 1.0047
RaM-ListNet Epoch 1240/200 | Loss: 5.6541 | Pred Std: 0.9173
RaM-ListNet Epoch 1260/200 | Loss: 5.5749 | Pred Std: 1.0343
RaM-ListNet Epoch 1280/200 | Loss: 5.9701 | Pred Std: 0.9508
RaM-ListNet Epoch 1300/200 | Loss: 5.7106 | Pred Std: 0.8761
RaM-ListNet Epoch 1320/200 | Loss: 5.5992 | Pred Std: 0.9381
RaM-ListNet Epoch 1340/200 | Loss: 5.6592 | Pred Std: 0.9899
RaM-ListNet Epoch 1360/200 | Loss: 5.7694 | Pred Std: 0.9842
RaM-ListNet Epoch 1380/200 | Loss: 5.8379 | Pred Std: 0.9883
RaM-ListNet Epoch 1400/200 | Loss: 5.6011 | Pred Std: 0.9188
RaM-ListNet Epoch 1420/200 | Loss: 6.0072 | Pred Std: 0.9906
RaM-ListNet Epoch 1440/200 | Loss: 5.6088 | Pred Std: 0.9950
RaM-ListNet Epoch 1460/200 | Loss: 5.9491 | Pred Std: 1.0115
RaM-ListNet Epoch 1480/200 | Loss: 5.8529 | Pred Std: 0.9508
RaM-ListNet Epoch 1500/200 | Loss: 5.6672 | Pred Std: 0.9974
RaM-ListNet Epoch 1520/200 | Loss: 5.8226 | Pred Std: 0.9882
RaM-ListNet Epoch 1540/200 | Loss: 5.7143 | Pred Std: 1.0060
RaM-ListNet Epoch 1560/200 | Loss: 5.5647 | Pred Std: 0.9182
RaM-ListNet Epoch 1580/200 | Loss: 5.8427 | Pred Std: 0.9959
RaM-ListNet Epoch 1600/200 | Loss: 5.5952 | Pred Std: 0.9831
RaM-ListNet Epoch 1620/200 | Loss: 5.9899 | Pred Std: 0.9506
RaM-ListNet Epoch 1640/200 | Loss: 5.8737 | Pred Std: 0.9882
RaM-ListNet Epoch 1660/200 | Loss: 5.8629 | Pred Std: 0.9554
RaM-ListNet Epoch 1680/200 | Loss: 5.8672 | Pred Std: 1.0159
RaM-ListNet Epoch 1700/200 | Loss: 5.7260 | Pred Std: 0.8831
RaM-ListNet Epoch 1720/200 | Loss: 5.8017 | Pred Std: 0.9802
RaM-ListNet Epoch 1740/200 | Loss: 5.7176 | Pred Std: 0.9817
RaM-ListNet Epoch 1760/200 | Loss: 5.4510 | Pred Std: 1.0004
RaM-ListNet Epoch 1780/200 | Loss: 5.7663 | Pred Std: 0.9924
RaM-ListNet Epoch 1800/200 | Loss: 5.8739 | Pred Std: 0.9378
RaM-ListNet Epoch 1820/200 | Loss: 5.6973 | Pred Std: 0.9842
RaM-ListNet Epoch 1840/200 | Loss: 5.6556 | Pred Std: 1.0468
RaM-ListNet Epoch 1860/200 | Loss: 5.9235 | Pred Std: 1.0228
RaM-ListNet Epoch 1880/200 | Loss: 5.8038 | Pred Std: 0.9927
RaM-ListNet Epoch 1900/200 | Loss: 5.6631 | Pred Std: 1.0445
RaM-ListNet Epoch 1920/200 | Loss: 5.5075 | Pred Std: 1.0040
RaM-ListNet Epoch 1940/200 | Loss: 5.7286 | Pred Std: 1.0151
RaM-ListNet Epoch 1960/200 | Loss: 5.6973 | Pred Std: 0.9724
RaM-ListNet Epoch 1980/200 | Loss: 5.5604 | Pred Std: 0.9426
RaM-ListNet Epoch 2000/200 | Loss: 5.8940 | Pred Std: 1.0355
Performing Output Adaptation...
Proxy Statistics: Mean=-0.4025, Std=1.0654
These will be used to normalize gradients during sampling.

Start Training Flow Model...
Flow Epoch 10/100 | Loss: 0.417529
Flow Epoch 20/100 | Loss: 0.313626
Flow Epoch 30/100 | Loss: 0.265150
Flow Epoch 40/100 | Loss: 0.242068
Flow Epoch 50/100 | Loss: 0.226311
Flow Epoch 60/100 | Loss: 0.216316
Flow Epoch 70/100 | Loss: 0.209364
Flow Epoch 80/100 | Loss: 0.200890
Flow Epoch 90/100 | Loss: 0.197630
Flow Epoch 100/100 | Loss: 0.191658
DEBUG: Top 5 indices: [17123 11687 28136 32111 15662]
DEBUG: Start Scores Mean: 0.2229 (Should be > 0.9)
normalized_scores: [0.8105286  0.17622562 0.8105286  0.7739352  0.8792624  0.86456317
 0.86456317 0.60551274 0.26317364 0.87806666 0.8105286  0.28730467
 0.62861294 0.5104111  0.61681026 0.26561663 0.44608915 0.74013525
 0.80030304 0.59582317 0.75099987 0.397858   0.38570487 0.43084362
 0.44802704 0.44596544 0.80030304 0.29891148 0.39857957 0.3223828
 0.75099987 0.5702284  0.74013525 0.44802704 0.9248959  0.6239641
 0.8028285  0.27901703 0.56344575 0.6031213  0.86456317 0.59692615
 0.859069   0.33986518 0.42951387 0.44269782 0.4462747  0.8807261
 0.69739825 0.9295139  0.45039788 0.59692615 0.5271513  0.9355441
 0.41786584 0.29413888 0.5061951  0.40522823 0.849163   0.75099987
 0.42036036 0.8432771  0.95489216 0.3021585  0.35822374 0.859069
 0.5738156  0.48424938 0.86456317 0.4899394  0.2758731  0.7157362
 0.86456317 0.82555765 0.4778687  0.98835194 0.37782955 0.7054385
 0.5328104  0.3473591  0.5444275  0.86456317 0.5346452  0.7293737
 0.5011648  0.7962623  0.9355441  0.91982436 0.5764751  0.62231475
 0.9130623  0.25314394 0.7814394  0.6185214  0.52337855 0.5185544
 0.3308972  0.4899394  0.91819566 0.91819566 0.7962623  0.4026409
 0.33676246 0.5875459  0.566868   0.54885995 0.91819566 0.4093205
 0.59582317 0.47135407 0.33258772 0.8792624  0.8105286  0.86456317
 0.9355441  0.6454253  0.86456317 0.7739352  0.75099987 0.2630293
 0.36829466 0.5122356  0.23677483 0.8407826  0.7358265  0.30297282
 0.58879316 0.3252175 ]
------------------------------
Optimization Results (Proxy Screened Top 128):
Optimized Score Mean (valid only): 0.6098
Optimized Score Max (valid only):  0.9884
Normalized Percentile Scores (100th / 80th / 50th): 0.9884 | 0.8551 | 0.5958
------------------------------
Seed: 2 completed
Experiment: TFBind8-Exact-v0 | Device: cuda
Loading task: TFBind8-Exact-v0...
Data Processed (One-Hot+Noise). X_dim=32. Y_norm Mean=0.00, Std=1.00
Starting Dynamic All-to-Better Pairing (Total: 32898)...
Pairing Result: Total 32898
  - Evolving (Found better): 32662 (99.3%)
  - Staying (Identity map):  236 (0.7%) -> 关键! 保护精英不崩塌
Model Input Dimension: 32

Training RankProxy with ListNet Loss (RaM Strategy)...
RaM-ListNet Epoch 20/200 | Loss: 6.2354 | Pred Std: 0.6372
RaM-ListNet Epoch 40/200 | Loss: 6.1363 | Pred Std: 0.6205
RaM-ListNet Epoch 60/200 | Loss: 6.0404 | Pred Std: 0.7166
RaM-ListNet Epoch 80/200 | Loss: 5.8757 | Pred Std: 0.6689
RaM-ListNet Epoch 100/200 | Loss: 6.0428 | Pred Std: 0.7504
RaM-ListNet Epoch 120/200 | Loss: 6.1285 | Pred Std: 0.7221
RaM-ListNet Epoch 140/200 | Loss: 6.0354 | Pred Std: 0.6928
RaM-ListNet Epoch 160/200 | Loss: 5.8530 | Pred Std: 0.7863
RaM-ListNet Epoch 180/200 | Loss: 6.0878 | Pred Std: 0.7683
RaM-ListNet Epoch 200/200 | Loss: 5.9657 | Pred Std: 0.6732
RaM-ListNet Epoch 220/200 | Loss: 6.0351 | Pred Std: 0.7973
RaM-ListNet Epoch 240/200 | Loss: 6.1391 | Pred Std: 0.7316
RaM-ListNet Epoch 260/200 | Loss: 5.8716 | Pred Std: 0.7344
RaM-ListNet Epoch 280/200 | Loss: 5.9077 | Pred Std: 0.7780
RaM-ListNet Epoch 300/200 | Loss: 6.1743 | Pred Std: 0.8079
RaM-ListNet Epoch 320/200 | Loss: 5.9259 | Pred Std: 0.7220
RaM-ListNet Epoch 340/200 | Loss: 5.9487 | Pred Std: 0.7965
RaM-ListNet Epoch 360/200 | Loss: 5.8990 | Pred Std: 0.7954
RaM-ListNet Epoch 380/200 | Loss: 6.1223 | Pred Std: 0.7715
RaM-ListNet Epoch 400/200 | Loss: 5.9410 | Pred Std: 0.8414
RaM-ListNet Epoch 420/200 | Loss: 5.8242 | Pred Std: 0.8627
RaM-ListNet Epoch 440/200 | Loss: 5.7867 | Pred Std: 0.8995
RaM-ListNet Epoch 460/200 | Loss: 5.9372 | Pred Std: 0.7757
RaM-ListNet Epoch 480/200 | Loss: 6.0423 | Pred Std: 0.8447
RaM-ListNet Epoch 500/200 | Loss: 5.9528 | Pred Std: 0.8684
RaM-ListNet Epoch 520/200 | Loss: 5.8102 | Pred Std: 0.8607
RaM-ListNet Epoch 540/200 | Loss: 5.8323 | Pred Std: 0.8977
RaM-ListNet Epoch 560/200 | Loss: 5.9109 | Pred Std: 0.9049
RaM-ListNet Epoch 580/200 | Loss: 5.8814 | Pred Std: 0.8787
RaM-ListNet Epoch 600/200 | Loss: 5.8783 | Pred Std: 0.8763
RaM-ListNet Epoch 620/200 | Loss: 6.0495 | Pred Std: 0.8917
RaM-ListNet Epoch 640/200 | Loss: 5.9754 | Pred Std: 0.7886
RaM-ListNet Epoch 660/200 | Loss: 6.0589 | Pred Std: 0.8491
RaM-ListNet Epoch 680/200 | Loss: 5.7493 | Pred Std: 0.8762
RaM-ListNet Epoch 700/200 | Loss: 5.8932 | Pred Std: 0.8924
RaM-ListNet Epoch 720/200 | Loss: 5.9582 | Pred Std: 0.9142
RaM-ListNet Epoch 740/200 | Loss: 5.8593 | Pred Std: 0.8668
RaM-ListNet Epoch 760/200 | Loss: 5.8180 | Pred Std: 0.9615
RaM-ListNet Epoch 780/200 | Loss: 5.9692 | Pred Std: 0.8577
RaM-ListNet Epoch 800/200 | Loss: 5.8627 | Pred Std: 0.8379
RaM-ListNet Epoch 820/200 | Loss: 5.7684 | Pred Std: 0.9275
RaM-ListNet Epoch 840/200 | Loss: 5.8558 | Pred Std: 0.8907
RaM-ListNet Epoch 860/200 | Loss: 6.1095 | Pred Std: 0.8552
RaM-ListNet Epoch 880/200 | Loss: 5.7291 | Pred Std: 0.8170
RaM-ListNet Epoch 900/200 | Loss: 5.8480 | Pred Std: 0.8537
RaM-ListNet Epoch 920/200 | Loss: 6.0378 | Pred Std: 0.8709
RaM-ListNet Epoch 940/200 | Loss: 5.6782 | Pred Std: 0.9014
RaM-ListNet Epoch 960/200 | Loss: 5.8725 | Pred Std: 0.9482
RaM-ListNet Epoch 980/200 | Loss: 5.9780 | Pred Std: 0.8319
RaM-ListNet Epoch 1000/200 | Loss: 5.7035 | Pred Std: 0.8635
RaM-ListNet Epoch 1020/200 | Loss: 5.7784 | Pred Std: 0.9214
RaM-ListNet Epoch 1040/200 | Loss: 5.6410 | Pred Std: 1.0054
RaM-ListNet Epoch 1060/200 | Loss: 5.9820 | Pred Std: 0.9575
RaM-ListNet Epoch 1080/200 | Loss: 5.8149 | Pred Std: 0.8928
RaM-ListNet Epoch 1100/200 | Loss: 5.7817 | Pred Std: 1.0283
RaM-ListNet Epoch 1120/200 | Loss: 5.8051 | Pred Std: 0.9009
RaM-ListNet Epoch 1140/200 | Loss: 5.8734 | Pred Std: 0.8920
RaM-ListNet Epoch 1160/200 | Loss: 6.1220 | Pred Std: 0.8832
RaM-ListNet Epoch 1180/200 | Loss: 5.8605 | Pred Std: 0.9280
RaM-ListNet Epoch 1200/200 | Loss: 5.6791 | Pred Std: 0.9623
RaM-ListNet Epoch 1220/200 | Loss: 5.7843 | Pred Std: 0.9405
RaM-ListNet Epoch 1240/200 | Loss: 5.8514 | Pred Std: 0.9686
RaM-ListNet Epoch 1260/200 | Loss: 5.8678 | Pred Std: 0.8385
RaM-ListNet Epoch 1280/200 | Loss: 5.7017 | Pred Std: 0.9054
RaM-ListNet Epoch 1300/200 | Loss: 5.7350 | Pred Std: 0.9066
RaM-ListNet Epoch 1320/200 | Loss: 5.8498 | Pred Std: 0.9353
RaM-ListNet Epoch 1340/200 | Loss: 5.9063 | Pred Std: 0.8891
RaM-ListNet Epoch 1360/200 | Loss: 5.6313 | Pred Std: 0.8577
RaM-ListNet Epoch 1380/200 | Loss: 5.7838 | Pred Std: 0.9651
RaM-ListNet Epoch 1400/200 | Loss: 5.7999 | Pred Std: 0.9426
RaM-ListNet Epoch 1420/200 | Loss: 5.8072 | Pred Std: 1.0590
RaM-ListNet Epoch 1440/200 | Loss: 5.5742 | Pred Std: 1.0264
RaM-ListNet Epoch 1460/200 | Loss: 5.9518 | Pred Std: 0.8889
RaM-ListNet Epoch 1480/200 | Loss: 5.6451 | Pred Std: 1.0233
RaM-ListNet Epoch 1500/200 | Loss: 5.7063 | Pred Std: 0.9698
RaM-ListNet Epoch 1520/200 | Loss: 5.7031 | Pred Std: 0.9889
RaM-ListNet Epoch 1540/200 | Loss: 5.7608 | Pred Std: 1.0036
RaM-ListNet Epoch 1560/200 | Loss: 5.6581 | Pred Std: 0.8931
RaM-ListNet Epoch 1580/200 | Loss: 5.8182 | Pred Std: 0.9295
RaM-ListNet Epoch 1600/200 | Loss: 5.7238 | Pred Std: 0.9138
RaM-ListNet Epoch 1620/200 | Loss: 5.7790 | Pred Std: 1.0308
RaM-ListNet Epoch 1640/200 | Loss: 5.5389 | Pred Std: 0.9509
RaM-ListNet Epoch 1660/200 | Loss: 5.5854 | Pred Std: 0.9324
RaM-ListNet Epoch 1680/200 | Loss: 5.6454 | Pred Std: 1.0529
RaM-ListNet Epoch 1700/200 | Loss: 5.7090 | Pred Std: 0.9904
RaM-ListNet Epoch 1720/200 | Loss: 5.7132 | Pred Std: 0.9741
RaM-ListNet Epoch 1740/200 | Loss: 5.6853 | Pred Std: 1.0419
RaM-ListNet Epoch 1760/200 | Loss: 5.7352 | Pred Std: 0.9090
RaM-ListNet Epoch 1780/200 | Loss: 5.5999 | Pred Std: 1.0104
RaM-ListNet Epoch 1800/200 | Loss: 5.8536 | Pred Std: 0.9624
RaM-ListNet Epoch 1820/200 | Loss: 5.6537 | Pred Std: 0.9502
RaM-ListNet Epoch 1840/200 | Loss: 5.6423 | Pred Std: 0.9603
RaM-ListNet Epoch 1860/200 | Loss: 5.7616 | Pred Std: 1.0106
RaM-ListNet Epoch 1880/200 | Loss: 5.7491 | Pred Std: 0.9933
RaM-ListNet Epoch 1900/200 | Loss: 5.4535 | Pred Std: 1.0265
RaM-ListNet Epoch 1920/200 | Loss: 5.5189 | Pred Std: 0.9495
RaM-ListNet Epoch 1940/200 | Loss: 5.5224 | Pred Std: 0.9476
RaM-ListNet Epoch 1960/200 | Loss: 5.6748 | Pred Std: 0.9513
RaM-ListNet Epoch 1980/200 | Loss: 5.7557 | Pred Std: 1.0321
RaM-ListNet Epoch 2000/200 | Loss: 5.7923 | Pred Std: 0.9514
Performing Output Adaptation...
Proxy Statistics: Mean=-1.0718, Std=1.0138
These will be used to normalize gradients during sampling.

Start Training Flow Model...
Flow Epoch 10/100 | Loss: 0.414460
Flow Epoch 20/100 | Loss: 0.310243
Flow Epoch 30/100 | Loss: 0.265081
Flow Epoch 40/100 | Loss: 0.240777
Flow Epoch 50/100 | Loss: 0.223273
Flow Epoch 60/100 | Loss: 0.214156
Flow Epoch 70/100 | Loss: 0.206925
Flow Epoch 80/100 | Loss: 0.201975
Flow Epoch 90/100 | Loss: 0.195672
Flow Epoch 100/100 | Loss: 0.191359
DEBUG: Top 5 indices: [17123 11687 28136 32111 15662]
DEBUG: Start Scores Mean: 0.1596 (Should be > 0.9)
normalized_scores: [0.57902116 0.4383169  0.41865954 0.8028285  0.9326063  0.8382159
 0.83613366 0.328279   0.53137755 0.7959325  0.47226116 0.33260834
 0.47872427 0.8231765  0.38478744 0.374665   0.5346452  0.38928175
 0.41685563 0.6715458  0.95364493 0.45758256 0.6201501  0.1828949
 0.2458871  0.22370428 0.37047994 0.23691915 0.8359069  0.43535852
 0.30711666 0.49400073 0.9487074  0.5292026  0.6868119  0.8347215
 0.39402342 0.53598523 0.70827323 0.45758256 0.6713087  0.5381705
 0.5770729  0.509638   0.4191028  0.37047994 0.6392302  0.33354637
 0.4742403  0.8359069  0.2274461  0.5337484  0.40260997 0.328279
 0.6762153  0.3275574  0.36837712 0.6279945  0.43269905 0.48021895
 0.73000246 0.4725807  0.34795696 0.4719313  0.41668043 0.33892715
 0.33980334 0.13634396 0.38712737 0.43860555 0.76859564 0.47135407
 0.66269124 0.5672082  0.37552056 0.79195356 0.4649528  0.40721765
 0.52858406 0.5057312  0.5337484  0.46861213 0.48050755 0.58028907
 0.5353977  0.7124376  0.35255432 0.79511815 0.46897292 0.7496083
 0.37538654 0.54553044 0.18897662 0.22997154 0.76479197 0.54424196
 0.7182307  0.28198573 0.43303922 0.9179277  0.5282542  0.47135407
 0.5587453  0.31687832 0.2134272  0.41355708 0.5994413  0.4191028
 0.76859564 0.8164454  0.32455778 0.88546777 0.3116934  0.38423082
 0.5795778  0.41685563 0.36775863 0.21459201 0.7772853  0.76488477
 0.42447326 0.39035377 0.53598523 0.63016945 0.7784089  0.50987506
 0.45307797 0.2639055 ]
------------------------------
Optimization Results (Proxy Screened Top 128):
Optimized Score Mean (valid only): 0.5159
Optimized Score Max (valid only):  0.9536
Normalized Percentile Scores (100th / 80th / 50th): 0.9536 | 0.7108 | 0.4734
------------------------------
Seed: 3 completed
Experiment: TFBind8-Exact-v0 | Device: cuda
Loading task: TFBind8-Exact-v0...
Data Processed (One-Hot+Noise). X_dim=32. Y_norm Mean=0.00, Std=1.00
Starting Dynamic All-to-Better Pairing (Total: 32898)...
Pairing Result: Total 32898
  - Evolving (Found better): 32656 (99.3%)
  - Staying (Identity map):  242 (0.7%) -> 关键! 保护精英不崩塌
Model Input Dimension: 32

Training RankProxy with ListNet Loss (RaM Strategy)...
RaM-ListNet Epoch 20/200 | Loss: 6.1444 | Pred Std: 0.6657
RaM-ListNet Epoch 40/200 | Loss: 6.1422 | Pred Std: 0.6006
RaM-ListNet Epoch 60/200 | Loss: 6.0712 | Pred Std: 0.6130
RaM-ListNet Epoch 80/200 | Loss: 5.9610 | Pred Std: 0.6927
RaM-ListNet Epoch 100/200 | Loss: 6.0818 | Pred Std: 0.6669
RaM-ListNet Epoch 120/200 | Loss: 6.1795 | Pred Std: 0.7075
RaM-ListNet Epoch 140/200 | Loss: 6.0398 | Pred Std: 0.7071
RaM-ListNet Epoch 160/200 | Loss: 5.9602 | Pred Std: 0.6786
RaM-ListNet Epoch 180/200 | Loss: 6.0381 | Pred Std: 0.6952
RaM-ListNet Epoch 200/200 | Loss: 6.0367 | Pred Std: 0.7360
RaM-ListNet Epoch 220/200 | Loss: 5.9863 | Pred Std: 0.8233
RaM-ListNet Epoch 240/200 | Loss: 5.9423 | Pred Std: 0.7635
RaM-ListNet Epoch 260/200 | Loss: 5.9884 | Pred Std: 0.7010
RaM-ListNet Epoch 280/200 | Loss: 5.9284 | Pred Std: 0.8276
RaM-ListNet Epoch 300/200 | Loss: 5.8372 | Pred Std: 0.8346
RaM-ListNet Epoch 320/200 | Loss: 6.0559 | Pred Std: 0.7505
RaM-ListNet Epoch 340/200 | Loss: 5.9027 | Pred Std: 0.8562
RaM-ListNet Epoch 360/200 | Loss: 6.0744 | Pred Std: 0.8546
RaM-ListNet Epoch 380/200 | Loss: 6.0536 | Pred Std: 0.8107
RaM-ListNet Epoch 400/200 | Loss: 5.7528 | Pred Std: 0.7461
RaM-ListNet Epoch 420/200 | Loss: 5.7704 | Pred Std: 0.7937
RaM-ListNet Epoch 440/200 | Loss: 6.0591 | Pred Std: 0.8372
RaM-ListNet Epoch 460/200 | Loss: 5.9545 | Pred Std: 0.7783
RaM-ListNet Epoch 480/200 | Loss: 5.8667 | Pred Std: 0.7950
RaM-ListNet Epoch 500/200 | Loss: 6.0498 | Pred Std: 0.8149
RaM-ListNet Epoch 520/200 | Loss: 5.7934 | Pred Std: 0.8634
RaM-ListNet Epoch 540/200 | Loss: 5.9929 | Pred Std: 0.8291
RaM-ListNet Epoch 560/200 | Loss: 5.9788 | Pred Std: 0.8660
RaM-ListNet Epoch 580/200 | Loss: 5.9577 | Pred Std: 0.9106
RaM-ListNet Epoch 600/200 | Loss: 6.0377 | Pred Std: 0.8638
RaM-ListNet Epoch 620/200 | Loss: 5.8129 | Pred Std: 0.8220
RaM-ListNet Epoch 640/200 | Loss: 5.9516 | Pred Std: 0.7981
RaM-ListNet Epoch 660/200 | Loss: 6.0718 | Pred Std: 0.8793
RaM-ListNet Epoch 680/200 | Loss: 5.7814 | Pred Std: 0.8999
RaM-ListNet Epoch 700/200 | Loss: 5.8208 | Pred Std: 0.8153
RaM-ListNet Epoch 720/200 | Loss: 5.9110 | Pred Std: 0.9180
RaM-ListNet Epoch 740/200 | Loss: 5.9354 | Pred Std: 0.9113
RaM-ListNet Epoch 760/200 | Loss: 5.7394 | Pred Std: 0.8767
RaM-ListNet Epoch 780/200 | Loss: 5.5892 | Pred Std: 0.8947
RaM-ListNet Epoch 800/200 | Loss: 5.7696 | Pred Std: 0.8399
RaM-ListNet Epoch 820/200 | Loss: 5.6407 | Pred Std: 0.9452
RaM-ListNet Epoch 840/200 | Loss: 5.9321 | Pred Std: 0.9258
RaM-ListNet Epoch 860/200 | Loss: 5.7938 | Pred Std: 0.9199
RaM-ListNet Epoch 880/200 | Loss: 6.0906 | Pred Std: 0.8555
RaM-ListNet Epoch 900/200 | Loss: 5.9336 | Pred Std: 0.9518
RaM-ListNet Epoch 920/200 | Loss: 5.7647 | Pred Std: 0.8629
RaM-ListNet Epoch 940/200 | Loss: 5.9975 | Pred Std: 0.8366
RaM-ListNet Epoch 960/200 | Loss: 5.9911 | Pred Std: 0.9074
RaM-ListNet Epoch 980/200 | Loss: 5.8691 | Pred Std: 0.8183
RaM-ListNet Epoch 1000/200 | Loss: 5.7508 | Pred Std: 0.8662
RaM-ListNet Epoch 1020/200 | Loss: 5.9836 | Pred Std: 0.8510
RaM-ListNet Epoch 1040/200 | Loss: 5.7779 | Pred Std: 0.9071
RaM-ListNet Epoch 1060/200 | Loss: 5.5942 | Pred Std: 0.8970
RaM-ListNet Epoch 1080/200 | Loss: 5.6752 | Pred Std: 0.9095
RaM-ListNet Epoch 1100/200 | Loss: 5.6535 | Pred Std: 0.8761
RaM-ListNet Epoch 1120/200 | Loss: 5.9140 | Pred Std: 0.8391
RaM-ListNet Epoch 1140/200 | Loss: 6.0625 | Pred Std: 0.9114
RaM-ListNet Epoch 1160/200 | Loss: 5.6838 | Pred Std: 0.9891
RaM-ListNet Epoch 1180/200 | Loss: 5.7524 | Pred Std: 0.9683
RaM-ListNet Epoch 1200/200 | Loss: 5.8099 | Pred Std: 0.9179
RaM-ListNet Epoch 1220/200 | Loss: 5.7927 | Pred Std: 0.9618
RaM-ListNet Epoch 1240/200 | Loss: 5.7363 | Pred Std: 0.9404
RaM-ListNet Epoch 1260/200 | Loss: 5.8635 | Pred Std: 1.0282
RaM-ListNet Epoch 1280/200 | Loss: 5.8229 | Pred Std: 0.9254
RaM-ListNet Epoch 1300/200 | Loss: 5.5581 | Pred Std: 1.0877
RaM-ListNet Epoch 1320/200 | Loss: 5.8061 | Pred Std: 0.9467
RaM-ListNet Epoch 1340/200 | Loss: 5.6254 | Pred Std: 0.8970
RaM-ListNet Epoch 1360/200 | Loss: 6.0120 | Pred Std: 0.9954
RaM-ListNet Epoch 1380/200 | Loss: 5.7331 | Pred Std: 0.9458
RaM-ListNet Epoch 1400/200 | Loss: 5.7127 | Pred Std: 1.0183
RaM-ListNet Epoch 1420/200 | Loss: 5.5210 | Pred Std: 1.0408
RaM-ListNet Epoch 1440/200 | Loss: 5.8207 | Pred Std: 0.9640
RaM-ListNet Epoch 1460/200 | Loss: 5.7240 | Pred Std: 1.0233
RaM-ListNet Epoch 1480/200 | Loss: 5.7412 | Pred Std: 0.9587
RaM-ListNet Epoch 1500/200 | Loss: 5.8496 | Pred Std: 0.9348
RaM-ListNet Epoch 1520/200 | Loss: 5.6765 | Pred Std: 0.9086
RaM-ListNet Epoch 1540/200 | Loss: 5.6265 | Pred Std: 0.9202
RaM-ListNet Epoch 1560/200 | Loss: 5.5016 | Pred Std: 0.9212
RaM-ListNet Epoch 1580/200 | Loss: 5.4809 | Pred Std: 0.9270
RaM-ListNet Epoch 1600/200 | Loss: 5.5905 | Pred Std: 0.9690
RaM-ListNet Epoch 1620/200 | Loss: 5.6043 | Pred Std: 0.9465
RaM-ListNet Epoch 1640/200 | Loss: 5.9272 | Pred Std: 0.9012
RaM-ListNet Epoch 1660/200 | Loss: 5.8494 | Pred Std: 0.9855
RaM-ListNet Epoch 1680/200 | Loss: 5.9106 | Pred Std: 0.9211
RaM-ListNet Epoch 1700/200 | Loss: 5.7073 | Pred Std: 0.9721
RaM-ListNet Epoch 1720/200 | Loss: 5.8040 | Pred Std: 1.0125
RaM-ListNet Epoch 1740/200 | Loss: 5.9626 | Pred Std: 0.9500
RaM-ListNet Epoch 1760/200 | Loss: 5.7451 | Pred Std: 0.9507
RaM-ListNet Epoch 1780/200 | Loss: 5.7934 | Pred Std: 0.9279
RaM-ListNet Epoch 1800/200 | Loss: 5.8174 | Pred Std: 0.9606
RaM-ListNet Epoch 1820/200 | Loss: 5.6300 | Pred Std: 1.0023
RaM-ListNet Epoch 1840/200 | Loss: 5.9284 | Pred Std: 1.0649
RaM-ListNet Epoch 1860/200 | Loss: 5.7299 | Pred Std: 1.0679
RaM-ListNet Epoch 1880/200 | Loss: 5.6820 | Pred Std: 0.9991
RaM-ListNet Epoch 1900/200 | Loss: 5.7232 | Pred Std: 1.0951
RaM-ListNet Epoch 1920/200 | Loss: 5.5631 | Pred Std: 1.0746
RaM-ListNet Epoch 1940/200 | Loss: 5.9319 | Pred Std: 0.9553
RaM-ListNet Epoch 1960/200 | Loss: 5.6387 | Pred Std: 1.0238
RaM-ListNet Epoch 1980/200 | Loss: 5.5433 | Pred Std: 0.9788
RaM-ListNet Epoch 2000/200 | Loss: 5.7114 | Pred Std: 1.0482
Performing Output Adaptation...
Proxy Statistics: Mean=-0.4212, Std=1.1189
These will be used to normalize gradients during sampling.

Start Training Flow Model...
Flow Epoch 10/100 | Loss: 0.414665
Flow Epoch 20/100 | Loss: 0.309435
Flow Epoch 30/100 | Loss: 0.260851
Flow Epoch 40/100 | Loss: 0.235864
Flow Epoch 50/100 | Loss: 0.222650
Flow Epoch 60/100 | Loss: 0.212578
Flow Epoch 70/100 | Loss: 0.206011
Flow Epoch 80/100 | Loss: 0.199690
Flow Epoch 90/100 | Loss: 0.193756
Flow Epoch 100/100 | Loss: 0.188533
DEBUG: Top 5 indices: [17123 11687 28136 32111 15662]
DEBUG: Start Scores Mean: 0.0126 (Should be > 0.9)
normalized_scores: [0.63539565 0.2306725  0.45483032 0.5144003  0.43303922 0.5186575
 0.3325465  0.584103   0.38208675 0.252309   0.22478662 0.38895187
 0.21172638 0.2592875  0.22370428 0.40297076 0.55235434 0.14124025
 0.60557455 0.6860079  0.26552385 0.41870078 0.42895725 0.24682513
 0.5856595  0.8903847  0.20826289 0.35894528 0.23512556 0.32757804
 0.26774007 0.6045129  0.36267677 0.5302746  0.32833052 0.5047623
 0.33325773 0.717839   0.46801427 0.4096091  0.58401024 0.4563353
 0.31353852 0.3955593  0.5254917  0.584103   0.39075577 0.78460395
 0.2960974  0.403816   0.4125572  0.6313652  0.9042902  0.48401228
 0.66276336 0.30606523 0.6806993  0.7066136  0.19732611 0.87553084
 0.247155   0.47095203 0.43949202 0.24525832 0.47672454 0.5889993
 0.7488249  0.44078052 0.52364653 0.3484311  0.6522183  0.4728281
 0.35139984 0.660609   0.3987754  0.8709438  0.6947285  0.43679133
 0.41870078 0.8231765  0.82785636 0.30166373 0.7524945  0.3658001
 0.61153257 0.23793964 0.43052405 0.303447   0.75174206 0.6033583
 0.72403413 0.3963324  0.28198573 0.44304827 0.5444275  0.18136932
 0.6326331  0.54385024 0.5574053  0.72646683 0.38288048 0.44669732
 0.3089824  0.5334907  0.39050838 0.53153217 0.73161054 0.3889828
 0.5500454  0.34021565 0.49232054 0.32352698 0.5254917  0.17515358
 0.43221456 0.5750113  0.20133592 0.2913248  0.36020285 0.7622356
 0.48200223 0.5094112  0.32281575 0.7646167  0.2748423  0.3212283
 0.5192141  0.4591494 ]
------------------------------
Optimization Results (Proxy Screened Top 128):
Optimized Score Mean (valid only): 0.4691
Optimized Score Max (valid only):  0.9043
Normalized Percentile Scores (100th / 80th / 50th): 0.9043 | 0.6234 | 0.4419
------------------------------
Seed: 4 completed
Experiment: TFBind8-Exact-v0 | Device: cuda
Loading task: TFBind8-Exact-v0...
Data Processed (One-Hot+Noise). X_dim=32. Y_norm Mean=0.00, Std=1.00
Starting Dynamic All-to-Better Pairing (Total: 32898)...
Pairing Result: Total 32898
  - Evolving (Found better): 32657 (99.3%)
  - Staying (Identity map):  241 (0.7%) -> 关键! 保护精英不崩塌
Model Input Dimension: 32

Training RankProxy with ListNet Loss (RaM Strategy)...
RaM-ListNet Epoch 20/200 | Loss: 6.1159 | Pred Std: 0.7674
RaM-ListNet Epoch 40/200 | Loss: 6.0582 | Pred Std: 0.6411
RaM-ListNet Epoch 60/200 | Loss: 5.9769 | Pred Std: 0.6107
RaM-ListNet Epoch 80/200 | Loss: 6.0686 | Pred Std: 0.6286
RaM-ListNet Epoch 100/200 | Loss: 5.8530 | Pred Std: 0.7088
RaM-ListNet Epoch 120/200 | Loss: 5.9892 | Pred Std: 0.6472
RaM-ListNet Epoch 140/200 | Loss: 6.2418 | Pred Std: 0.7412
RaM-ListNet Epoch 160/200 | Loss: 5.9161 | Pred Std: 0.7317
RaM-ListNet Epoch 180/200 | Loss: 5.9681 | Pred Std: 0.8009
RaM-ListNet Epoch 200/200 | Loss: 5.9297 | Pred Std: 0.7682
RaM-ListNet Epoch 220/200 | Loss: 6.0317 | Pred Std: 0.7819
RaM-ListNet Epoch 240/200 | Loss: 5.9854 | Pred Std: 0.8271
RaM-ListNet Epoch 260/200 | Loss: 5.8395 | Pred Std: 0.7983
RaM-ListNet Epoch 280/200 | Loss: 5.9898 | Pred Std: 0.7542
RaM-ListNet Epoch 300/200 | Loss: 5.9317 | Pred Std: 0.7503
RaM-ListNet Epoch 320/200 | Loss: 5.9289 | Pred Std: 0.7831
RaM-ListNet Epoch 340/200 | Loss: 6.0479 | Pred Std: 0.7642
RaM-ListNet Epoch 360/200 | Loss: 5.9799 | Pred Std: 0.8141
RaM-ListNet Epoch 380/200 | Loss: 5.7718 | Pred Std: 0.7896
RaM-ListNet Epoch 400/200 | Loss: 5.8346 | Pred Std: 0.8272
RaM-ListNet Epoch 420/200 | Loss: 5.8399 | Pred Std: 0.7726
RaM-ListNet Epoch 440/200 | Loss: 5.9543 | Pred Std: 0.8655
RaM-ListNet Epoch 460/200 | Loss: 5.9151 | Pred Std: 0.8417
RaM-ListNet Epoch 480/200 | Loss: 5.8331 | Pred Std: 0.8187
RaM-ListNet Epoch 500/200 | Loss: 5.8547 | Pred Std: 0.7589
RaM-ListNet Epoch 520/200 | Loss: 5.8715 | Pred Std: 0.8173
RaM-ListNet Epoch 540/200 | Loss: 5.9422 | Pred Std: 0.8364
RaM-ListNet Epoch 560/200 | Loss: 5.9201 | Pred Std: 0.7884
RaM-ListNet Epoch 580/200 | Loss: 5.8272 | Pred Std: 0.8009
RaM-ListNet Epoch 600/200 | Loss: 6.0763 | Pred Std: 0.8116
RaM-ListNet Epoch 620/200 | Loss: 5.6861 | Pred Std: 0.8512
RaM-ListNet Epoch 640/200 | Loss: 5.9930 | Pred Std: 0.8902
RaM-ListNet Epoch 660/200 | Loss: 5.7591 | Pred Std: 0.8384
RaM-ListNet Epoch 680/200 | Loss: 6.0506 | Pred Std: 0.8419
RaM-ListNet Epoch 700/200 | Loss: 5.8818 | Pred Std: 0.7824
RaM-ListNet Epoch 720/200 | Loss: 6.0095 | Pred Std: 0.8255
RaM-ListNet Epoch 740/200 | Loss: 5.9864 | Pred Std: 0.8648
RaM-ListNet Epoch 760/200 | Loss: 5.7197 | Pred Std: 0.8447
RaM-ListNet Epoch 780/200 | Loss: 6.0586 | Pred Std: 0.8995
RaM-ListNet Epoch 800/200 | Loss: 5.8074 | Pred Std: 0.8570
RaM-ListNet Epoch 820/200 | Loss: 5.9538 | Pred Std: 0.8767
RaM-ListNet Epoch 840/200 | Loss: 5.8153 | Pred Std: 0.8698
RaM-ListNet Epoch 860/200 | Loss: 5.8684 | Pred Std: 0.8773
RaM-ListNet Epoch 880/200 | Loss: 5.6988 | Pred Std: 0.9457
RaM-ListNet Epoch 900/200 | Loss: 5.7157 | Pred Std: 0.8761
RaM-ListNet Epoch 920/200 | Loss: 5.6283 | Pred Std: 0.8940
RaM-ListNet Epoch 940/200 | Loss: 5.9123 | Pred Std: 0.8470
RaM-ListNet Epoch 960/200 | Loss: 6.0242 | Pred Std: 0.9494
RaM-ListNet Epoch 980/200 | Loss: 5.8175 | Pred Std: 0.8633
RaM-ListNet Epoch 1000/200 | Loss: 5.7734 | Pred Std: 0.8285
RaM-ListNet Epoch 1020/200 | Loss: 5.8895 | Pred Std: 0.9394
RaM-ListNet Epoch 1040/200 | Loss: 5.8974 | Pred Std: 0.8701
RaM-ListNet Epoch 1060/200 | Loss: 5.8340 | Pred Std: 0.9009
RaM-ListNet Epoch 1080/200 | Loss: 5.8165 | Pred Std: 0.9723
RaM-ListNet Epoch 1100/200 | Loss: 5.7197 | Pred Std: 0.8875
RaM-ListNet Epoch 1120/200 | Loss: 5.8579 | Pred Std: 0.9088
RaM-ListNet Epoch 1140/200 | Loss: 5.7851 | Pred Std: 0.8484
RaM-ListNet Epoch 1160/200 | Loss: 5.7135 | Pred Std: 0.9240
RaM-ListNet Epoch 1180/200 | Loss: 5.7991 | Pred Std: 0.9264
RaM-ListNet Epoch 1200/200 | Loss: 5.7390 | Pred Std: 0.9495
RaM-ListNet Epoch 1220/200 | Loss: 5.8880 | Pred Std: 0.8922
RaM-ListNet Epoch 1240/200 | Loss: 6.0284 | Pred Std: 0.9207
RaM-ListNet Epoch 1260/200 | Loss: 5.8857 | Pred Std: 0.8710
RaM-ListNet Epoch 1280/200 | Loss: 5.8996 | Pred Std: 0.9299
RaM-ListNet Epoch 1300/200 | Loss: 5.8807 | Pred Std: 0.9052
RaM-ListNet Epoch 1320/200 | Loss: 5.8526 | Pred Std: 0.9189
RaM-ListNet Epoch 1340/200 | Loss: 5.9050 | Pred Std: 0.8969
RaM-ListNet Epoch 1360/200 | Loss: 5.6043 | Pred Std: 0.9733
RaM-ListNet Epoch 1380/200 | Loss: 5.6368 | Pred Std: 0.9527
RaM-ListNet Epoch 1400/200 | Loss: 5.7932 | Pred Std: 0.8749
RaM-ListNet Epoch 1420/200 | Loss: 5.6378 | Pred Std: 0.9624
RaM-ListNet Epoch 1440/200 | Loss: 5.7936 | Pred Std: 0.9815
RaM-ListNet Epoch 1460/200 | Loss: 5.8826 | Pred Std: 0.8695
RaM-ListNet Epoch 1480/200 | Loss: 5.5907 | Pred Std: 0.9705
RaM-ListNet Epoch 1500/200 | Loss: 5.9093 | Pred Std: 0.9542
RaM-ListNet Epoch 1520/200 | Loss: 5.9509 | Pred Std: 1.0106
RaM-ListNet Epoch 1540/200 | Loss: 5.7163 | Pred Std: 0.9616
RaM-ListNet Epoch 1560/200 | Loss: 5.6696 | Pred Std: 0.9337
RaM-ListNet Epoch 1580/200 | Loss: 5.7455 | Pred Std: 0.9722
RaM-ListNet Epoch 1600/200 | Loss: 5.7432 | Pred Std: 0.9123
RaM-ListNet Epoch 1620/200 | Loss: 5.8763 | Pred Std: 1.0016
RaM-ListNet Epoch 1640/200 | Loss: 5.9029 | Pred Std: 0.9898
RaM-ListNet Epoch 1660/200 | Loss: 5.8024 | Pred Std: 1.0229
RaM-ListNet Epoch 1680/200 | Loss: 5.6276 | Pred Std: 0.9982
RaM-ListNet Epoch 1700/200 | Loss: 5.7554 | Pred Std: 0.9703
RaM-ListNet Epoch 1720/200 | Loss: 5.4976 | Pred Std: 1.0186
RaM-ListNet Epoch 1740/200 | Loss: 5.7401 | Pred Std: 0.9611
RaM-ListNet Epoch 1760/200 | Loss: 5.7427 | Pred Std: 1.0906
RaM-ListNet Epoch 1780/200 | Loss: 5.8606 | Pred Std: 1.0206
RaM-ListNet Epoch 1800/200 | Loss: 5.4301 | Pred Std: 1.0450
RaM-ListNet Epoch 1820/200 | Loss: 6.0022 | Pred Std: 0.9583
RaM-ListNet Epoch 1840/200 | Loss: 5.9542 | Pred Std: 1.0710
RaM-ListNet Epoch 1860/200 | Loss: 5.8080 | Pred Std: 1.0147
RaM-ListNet Epoch 1880/200 | Loss: 5.6803 | Pred Std: 0.9875
RaM-ListNet Epoch 1900/200 | Loss: 5.8166 | Pred Std: 0.8978
RaM-ListNet Epoch 1920/200 | Loss: 5.5374 | Pred Std: 1.0015
RaM-ListNet Epoch 1940/200 | Loss: 5.6914 | Pred Std: 1.0401
RaM-ListNet Epoch 1960/200 | Loss: 5.5786 | Pred Std: 0.9553
RaM-ListNet Epoch 1980/200 | Loss: 5.6997 | Pred Std: 0.9874
RaM-ListNet Epoch 2000/200 | Loss: 5.5880 | Pred Std: 1.0552
Performing Output Adaptation...
Proxy Statistics: Mean=-0.4126, Std=1.0888
These will be used to normalize gradients during sampling.

Start Training Flow Model...
Flow Epoch 10/100 | Loss: 0.423426
Flow Epoch 20/100 | Loss: 0.325339
Flow Epoch 30/100 | Loss: 0.273025
Flow Epoch 40/100 | Loss: 0.243863
Flow Epoch 50/100 | Loss: 0.227987
Flow Epoch 60/100 | Loss: 0.213946
Flow Epoch 70/100 | Loss: 0.206131
Flow Epoch 80/100 | Loss: 0.200370
Flow Epoch 90/100 | Loss: 0.192788
Flow Epoch 100/100 | Loss: 0.191235
DEBUG: Top 5 indices: [17123 11687 28136 32111 15662]
DEBUG: Start Scores Mean: 0.1752 (Should be > 0.9)
normalized_scores: [0.50948334 0.8940234  0.2861914  0.45758256 0.3349895  0.4336886
 0.45758256 0.35037935 0.25367996 0.3916732  0.364759   0.26474044
 0.55479735 0.634705   0.9161444  0.6465592  0.73021895 0.41067085
 0.40407372 0.44765595 0.31999135 0.28962398 0.2543706  0.849163
 0.4884035  0.5227704  0.17382386 0.5750113  0.25496846 0.7520307
 0.8460087  0.47135407 0.73021895 0.47135407 0.859069   0.9355441
 0.7258586  0.5818971  0.45758256 0.9355441  0.36530533 0.36982024
 0.31099245 0.5533233  0.9161444  0.6522183  0.859069   0.8662949
 0.63194245 0.95364493 0.5647343  0.9355441  0.35139984 0.5346452
 0.10579104 0.35822374 0.17776151 0.8164454  0.5589927  0.39680657
 0.5189564  0.5747124  0.58554614 0.45758256 0.32839236 0.9355441
 0.6021317  0.47135407 0.47065312 0.58554614 0.74306273 0.4299468
 0.849163   0.6882654  0.8460087  0.33892715 0.48023957 0.95364493
 0.5747124  0.47042635 0.24726838 0.4447697  0.849163   0.30514783
 0.41810292 0.7576485  0.99063003 0.73857874 0.46763286 0.37729353
 0.6522183  0.53327423 0.5672597  0.859069   0.1979755  0.6679689
 0.25496846 0.55479735 0.36223355 0.50071126 0.46889046 0.5404383
 0.9712304  0.37620088 0.1530223  0.8164454  0.21674637 0.7151383
 0.3370923  0.5467571  0.34082383 0.74013525 0.4215355  0.52427536
 0.25367996 0.39083824 0.45758256 0.71683913 0.74306273 0.5346452
 0.54321116 0.40977404 0.5750113  0.1446419  0.3706861  0.46557128
 0.482229   0.32018718]
------------------------------
Optimization Results (Proxy Screened Top 128):
Optimized Score Mean (valid only): 0.5371
Optimized Score Max (valid only):  0.9906
Normalized Percentile Scores (100th / 80th / 50th): 0.9906 | 0.7419 | 0.5051
------------------------------
Seed: 5 completed
Experiment: TFBind8-Exact-v0 | Device: cuda
Loading task: TFBind8-Exact-v0...
Data Processed (One-Hot+Noise). X_dim=32. Y_norm Mean=0.00, Std=1.00
Starting Dynamic All-to-Better Pairing (Total: 32898)...
Pairing Result: Total 32898
  - Evolving (Found better): 32668 (99.3%)
  - Staying (Identity map):  230 (0.7%) -> 关键! 保护精英不崩塌
Model Input Dimension: 32

Training RankProxy with ListNet Loss (RaM Strategy)...
RaM-ListNet Epoch 20/200 | Loss: 6.0191 | Pred Std: 0.6407
RaM-ListNet Epoch 40/200 | Loss: 6.2240 | Pred Std: 0.6585
RaM-ListNet Epoch 60/200 | Loss: 6.0964 | Pred Std: 0.5849
RaM-ListNet Epoch 80/200 | Loss: 6.1664 | Pred Std: 0.5648
RaM-ListNet Epoch 100/200 | Loss: 6.0519 | Pred Std: 0.6458
RaM-ListNet Epoch 120/200 | Loss: 6.2894 | Pred Std: 0.6895
RaM-ListNet Epoch 140/200 | Loss: 6.0896 | Pred Std: 0.6515
RaM-ListNet Epoch 160/200 | Loss: 6.2370 | Pred Std: 0.7080
RaM-ListNet Epoch 180/200 | Loss: 6.0321 | Pred Std: 0.7185
RaM-ListNet Epoch 200/200 | Loss: 6.0168 | Pred Std: 0.7933
RaM-ListNet Epoch 220/200 | Loss: 6.0588 | Pred Std: 0.7103
RaM-ListNet Epoch 240/200 | Loss: 5.9037 | Pred Std: 0.7983
RaM-ListNet Epoch 260/200 | Loss: 6.0265 | Pred Std: 0.7152
RaM-ListNet Epoch 280/200 | Loss: 5.9513 | Pred Std: 0.8231
RaM-ListNet Epoch 300/200 | Loss: 6.0089 | Pred Std: 0.8483
RaM-ListNet Epoch 320/200 | Loss: 5.8972 | Pred Std: 0.8438
RaM-ListNet Epoch 340/200 | Loss: 6.0025 | Pred Std: 0.7549
RaM-ListNet Epoch 360/200 | Loss: 5.8025 | Pred Std: 0.7548
RaM-ListNet Epoch 380/200 | Loss: 5.8587 | Pred Std: 0.7976
RaM-ListNet Epoch 400/200 | Loss: 5.7912 | Pred Std: 0.8194
RaM-ListNet Epoch 420/200 | Loss: 6.0951 | Pred Std: 0.8079
RaM-ListNet Epoch 440/200 | Loss: 5.8057 | Pred Std: 0.8776
RaM-ListNet Epoch 460/200 | Loss: 6.0097 | Pred Std: 0.8950
RaM-ListNet Epoch 480/200 | Loss: 6.0121 | Pred Std: 0.7911
RaM-ListNet Epoch 500/200 | Loss: 5.9828 | Pred Std: 0.8155
RaM-ListNet Epoch 520/200 | Loss: 5.9896 | Pred Std: 0.8647
RaM-ListNet Epoch 540/200 | Loss: 5.9187 | Pred Std: 0.8927
RaM-ListNet Epoch 560/200 | Loss: 6.0671 | Pred Std: 0.9428
RaM-ListNet Epoch 580/200 | Loss: 5.6695 | Pred Std: 0.9123
RaM-ListNet Epoch 600/200 | Loss: 5.8449 | Pred Std: 0.8617
RaM-ListNet Epoch 620/200 | Loss: 6.0364 | Pred Std: 0.9322
RaM-ListNet Epoch 640/200 | Loss: 5.8562 | Pred Std: 0.8388
RaM-ListNet Epoch 660/200 | Loss: 5.8692 | Pred Std: 0.8149
RaM-ListNet Epoch 680/200 | Loss: 5.9458 | Pred Std: 0.8702
RaM-ListNet Epoch 700/200 | Loss: 5.8026 | Pred Std: 0.8736
RaM-ListNet Epoch 720/200 | Loss: 6.0028 | Pred Std: 0.8703
RaM-ListNet Epoch 740/200 | Loss: 5.8517 | Pred Std: 0.8487
RaM-ListNet Epoch 760/200 | Loss: 5.6292 | Pred Std: 0.8372
RaM-ListNet Epoch 780/200 | Loss: 5.8396 | Pred Std: 0.9211
RaM-ListNet Epoch 800/200 | Loss: 5.8795 | Pred Std: 0.8181
RaM-ListNet Epoch 820/200 | Loss: 5.5304 | Pred Std: 0.8689
RaM-ListNet Epoch 840/200 | Loss: 5.9304 | Pred Std: 0.8178
RaM-ListNet Epoch 860/200 | Loss: 5.7735 | Pred Std: 0.8376
RaM-ListNet Epoch 880/200 | Loss: 6.0091 | Pred Std: 0.9977
RaM-ListNet Epoch 900/200 | Loss: 5.7311 | Pred Std: 0.9492
RaM-ListNet Epoch 920/200 | Loss: 5.7887 | Pred Std: 0.8753
RaM-ListNet Epoch 940/200 | Loss: 5.9117 | Pred Std: 0.9034
RaM-ListNet Epoch 960/200 | Loss: 5.8274 | Pred Std: 0.9756
RaM-ListNet Epoch 980/200 | Loss: 5.9829 | Pred Std: 0.8513
RaM-ListNet Epoch 1000/200 | Loss: 6.1454 | Pred Std: 0.8982
RaM-ListNet Epoch 1020/200 | Loss: 5.9731 | Pred Std: 0.8221
RaM-ListNet Epoch 1040/200 | Loss: 5.9411 | Pred Std: 0.8853
RaM-ListNet Epoch 1060/200 | Loss: 5.9950 | Pred Std: 0.9252
RaM-ListNet Epoch 1080/200 | Loss: 5.9362 | Pred Std: 0.9190
RaM-ListNet Epoch 1100/200 | Loss: 5.7912 | Pred Std: 0.9484
RaM-ListNet Epoch 1120/200 | Loss: 5.7837 | Pred Std: 0.8828
RaM-ListNet Epoch 1140/200 | Loss: 6.1583 | Pred Std: 0.9126
RaM-ListNet Epoch 1160/200 | Loss: 5.7445 | Pred Std: 0.9661
RaM-ListNet Epoch 1180/200 | Loss: 5.8989 | Pred Std: 0.8722
RaM-ListNet Epoch 1200/200 | Loss: 5.7586 | Pred Std: 1.0096
RaM-ListNet Epoch 1220/200 | Loss: 5.7821 | Pred Std: 0.8777
RaM-ListNet Epoch 1240/200 | Loss: 5.8508 | Pred Std: 0.9091
RaM-ListNet Epoch 1260/200 | Loss: 5.6327 | Pred Std: 0.8767
RaM-ListNet Epoch 1280/200 | Loss: 5.9332 | Pred Std: 0.8961
RaM-ListNet Epoch 1300/200 | Loss: 5.7960 | Pred Std: 0.9392
RaM-ListNet Epoch 1320/200 | Loss: 5.8310 | Pred Std: 0.8929
RaM-ListNet Epoch 1340/200 | Loss: 5.7869 | Pred Std: 0.9788
RaM-ListNet Epoch 1360/200 | Loss: 5.6281 | Pred Std: 0.9867
RaM-ListNet Epoch 1380/200 | Loss: 5.7316 | Pred Std: 0.9265
RaM-ListNet Epoch 1400/200 | Loss: 5.6907 | Pred Std: 1.0948
RaM-ListNet Epoch 1420/200 | Loss: 5.6674 | Pred Std: 1.0081
RaM-ListNet Epoch 1440/200 | Loss: 5.9618 | Pred Std: 0.9754
RaM-ListNet Epoch 1460/200 | Loss: 5.7206 | Pred Std: 0.9404
RaM-ListNet Epoch 1480/200 | Loss: 5.7360 | Pred Std: 1.0303
RaM-ListNet Epoch 1500/200 | Loss: 5.8201 | Pred Std: 0.9072
RaM-ListNet Epoch 1520/200 | Loss: 5.5091 | Pred Std: 1.0306
RaM-ListNet Epoch 1540/200 | Loss: 5.6014 | Pred Std: 1.0126
RaM-ListNet Epoch 1560/200 | Loss: 5.8289 | Pred Std: 0.9528
RaM-ListNet Epoch 1580/200 | Loss: 5.8671 | Pred Std: 0.9842
RaM-ListNet Epoch 1600/200 | Loss: 5.8933 | Pred Std: 0.9090
RaM-ListNet Epoch 1620/200 | Loss: 5.6998 | Pred Std: 0.9024
RaM-ListNet Epoch 1640/200 | Loss: 5.7559 | Pred Std: 0.9397
RaM-ListNet Epoch 1660/200 | Loss: 5.5168 | Pred Std: 0.9258
RaM-ListNet Epoch 1680/200 | Loss: 5.7855 | Pred Std: 1.0359
RaM-ListNet Epoch 1700/200 | Loss: 5.5672 | Pred Std: 0.9474
RaM-ListNet Epoch 1720/200 | Loss: 5.7028 | Pred Std: 1.0604
RaM-ListNet Epoch 1740/200 | Loss: 5.5443 | Pred Std: 1.0062
RaM-ListNet Epoch 1760/200 | Loss: 5.8408 | Pred Std: 0.9675
RaM-ListNet Epoch 1780/200 | Loss: 5.5211 | Pred Std: 1.0771
RaM-ListNet Epoch 1800/200 | Loss: 5.9187 | Pred Std: 0.8958
RaM-ListNet Epoch 1820/200 | Loss: 5.6083 | Pred Std: 0.9391
RaM-ListNet Epoch 1840/200 | Loss: 5.7445 | Pred Std: 1.0821
RaM-ListNet Epoch 1860/200 | Loss: 5.9443 | Pred Std: 1.0429
RaM-ListNet Epoch 1880/200 | Loss: 5.8860 | Pred Std: 0.9862
RaM-ListNet Epoch 1900/200 | Loss: 5.8459 | Pred Std: 1.0138
RaM-ListNet Epoch 1920/200 | Loss: 5.5702 | Pred Std: 1.0030
RaM-ListNet Epoch 1940/200 | Loss: 5.5408 | Pred Std: 0.9832
RaM-ListNet Epoch 1960/200 | Loss: 5.9381 | Pred Std: 0.9917
RaM-ListNet Epoch 1980/200 | Loss: 5.8374 | Pred Std: 1.0205
RaM-ListNet Epoch 2000/200 | Loss: 5.8188 | Pred Std: 1.0702
Performing Output Adaptation...
Proxy Statistics: Mean=-1.9603, Std=1.1051
These will be used to normalize gradients during sampling.

Start Training Flow Model...
Flow Epoch 10/100 | Loss: 0.413535
Flow Epoch 20/100 | Loss: 0.314390
Flow Epoch 30/100 | Loss: 0.266479
Flow Epoch 40/100 | Loss: 0.240031
Flow Epoch 50/100 | Loss: 0.225127
Flow Epoch 60/100 | Loss: 0.215041
Flow Epoch 70/100 | Loss: 0.206102
Flow Epoch 80/100 | Loss: 0.201531
Flow Epoch 90/100 | Loss: 0.196290
Flow Epoch 100/100 | Loss: 0.190588
DEBUG: Top 5 indices: [17123 11687 28136 32111 15662]
DEBUG: Start Scores Mean: 0.0988 (Should be > 0.9)
normalized_scores: [0.665227   0.44316167 0.38931265 0.71825135 0.4157321  0.24017647
 0.5946996  0.57296    0.54913825 0.35791448 0.54050016 0.3485445
 0.2398363  0.33005196 0.30430257 0.52364653 0.37810788 0.64266276
 0.3767266  0.6694636  0.36015132 0.5206366  0.60824436 0.5496743
 0.5352225  0.7328475  0.18558529 0.4725807  0.8412877  0.7287552
 0.5628376  0.37864387 0.7112419  0.7575764  0.44446048 0.31790912
 0.2838618  0.676205   0.70371705 0.57562983 0.7372387  0.45183071
 0.5433142  0.43902817 0.41777307 0.5391189  0.8045396  0.3631097
 0.5099782  0.5202655  0.40981528 0.45758256 0.5634973  0.47932214
 0.66428894 0.45988125 0.546386   0.38984868 0.3073125  0.44455326
 0.60918236 0.33892715 0.455923   0.5389024  0.33747372 0.39236382
 0.6310147  0.727611   0.720715   0.56689894 0.34411207 0.42977157
 0.21397352 0.2632561  0.42859647 0.47535357 0.5176267  0.7233641
 0.56741434 0.2968705  0.6209026  0.49282563 0.51318395 0.7435472
 0.47585866 0.57909334 0.12954068 0.5459325  0.25584465 0.5730219
 0.63055086 0.506525   0.70472723 0.2606791  0.46477756 0.52440935
 0.67237043 0.59368944 0.46217993 0.8431225  0.95283055 0.71171606
 0.506525   0.33892715 0.43929616 0.82711416 0.4081763  0.24243392
 0.504618   0.20794335 0.553447   0.3575434  0.4043005  0.3972292
 0.40600133 0.5941842  0.53908795 0.66269124 0.9523461  0.71171606
 0.68338966 0.48023957 0.5471076  0.3073125  0.30697232 0.5090092
 0.44574898 0.4235146 ]
------------------------------
Optimization Results (Proxy Screened Top 128):
Optimized Score Mean (valid only): 0.5070
Optimized Score Max (valid only):  0.9528
Normalized Percentile Scores (100th / 80th / 50th): 0.9528 | 0.6636 | 0.5078
------------------------------
Seed: 6 completed
Experiment: TFBind8-Exact-v0 | Device: cuda
Loading task: TFBind8-Exact-v0...
Data Processed (One-Hot+Noise). X_dim=32. Y_norm Mean=0.00, Std=1.00
Starting Dynamic All-to-Better Pairing (Total: 32898)...
Pairing Result: Total 32898
  - Evolving (Found better): 32663 (99.3%)
  - Staying (Identity map):  235 (0.7%) -> 关键! 保护精英不崩塌
Model Input Dimension: 32

Training RankProxy with ListNet Loss (RaM Strategy)...
RaM-ListNet Epoch 20/200 | Loss: 6.0896 | Pred Std: 0.7583
RaM-ListNet Epoch 40/200 | Loss: 6.1163 | Pred Std: 0.6658
RaM-ListNet Epoch 60/200 | Loss: 6.0949 | Pred Std: 0.6444
RaM-ListNet Epoch 80/200 | Loss: 6.2193 | Pred Std: 0.5655
RaM-ListNet Epoch 100/200 | Loss: 5.9566 | Pred Std: 0.6734
RaM-ListNet Epoch 120/200 | Loss: 6.0969 | Pred Std: 0.6938
RaM-ListNet Epoch 140/200 | Loss: 6.0263 | Pred Std: 0.7705
RaM-ListNet Epoch 160/200 | Loss: 5.8880 | Pred Std: 0.7318
RaM-ListNet Epoch 180/200 | Loss: 5.7853 | Pred Std: 0.6045
RaM-ListNet Epoch 200/200 | Loss: 6.0428 | Pred Std: 0.7441
RaM-ListNet Epoch 220/200 | Loss: 5.9580 | Pred Std: 0.7798
RaM-ListNet Epoch 240/200 | Loss: 5.9910 | Pred Std: 0.6882
RaM-ListNet Epoch 260/200 | Loss: 6.0291 | Pred Std: 0.7824
RaM-ListNet Epoch 280/200 | Loss: 6.0301 | Pred Std: 0.7708
RaM-ListNet Epoch 300/200 | Loss: 5.8910 | Pred Std: 0.7522
RaM-ListNet Epoch 320/200 | Loss: 6.0281 | Pred Std: 0.8182
RaM-ListNet Epoch 340/200 | Loss: 5.8369 | Pred Std: 0.8025
RaM-ListNet Epoch 360/200 | Loss: 5.9066 | Pred Std: 0.7652
RaM-ListNet Epoch 380/200 | Loss: 6.0524 | Pred Std: 0.7933
RaM-ListNet Epoch 400/200 | Loss: 5.9275 | Pred Std: 0.7446
RaM-ListNet Epoch 420/200 | Loss: 5.9503 | Pred Std: 0.8506
RaM-ListNet Epoch 440/200 | Loss: 5.7997 | Pred Std: 0.7879
RaM-ListNet Epoch 460/200 | Loss: 5.8607 | Pred Std: 0.8094
RaM-ListNet Epoch 480/200 | Loss: 5.9526 | Pred Std: 0.8449
RaM-ListNet Epoch 500/200 | Loss: 5.8328 | Pred Std: 0.8927
RaM-ListNet Epoch 520/200 | Loss: 5.9464 | Pred Std: 0.8487
RaM-ListNet Epoch 540/200 | Loss: 6.0348 | Pred Std: 0.8542
RaM-ListNet Epoch 560/200 | Loss: 6.1066 | Pred Std: 0.8549
RaM-ListNet Epoch 580/200 | Loss: 6.0517 | Pred Std: 0.8362
RaM-ListNet Epoch 600/200 | Loss: 5.9423 | Pred Std: 0.7607
RaM-ListNet Epoch 620/200 | Loss: 5.6128 | Pred Std: 0.7699
RaM-ListNet Epoch 640/200 | Loss: 5.9831 | Pred Std: 0.7975
RaM-ListNet Epoch 660/200 | Loss: 5.8020 | Pred Std: 0.9325
RaM-ListNet Epoch 680/200 | Loss: 5.7325 | Pred Std: 0.8904
RaM-ListNet Epoch 700/200 | Loss: 5.9347 | Pred Std: 0.8857
RaM-ListNet Epoch 720/200 | Loss: 5.8937 | Pred Std: 0.8398
RaM-ListNet Epoch 740/200 | Loss: 5.9257 | Pred Std: 0.8820
RaM-ListNet Epoch 760/200 | Loss: 5.7197 | Pred Std: 0.7760
RaM-ListNet Epoch 780/200 | Loss: 5.9559 | Pred Std: 0.9186
RaM-ListNet Epoch 800/200 | Loss: 6.0318 | Pred Std: 0.8997
RaM-ListNet Epoch 820/200 | Loss: 6.0123 | Pred Std: 0.8779
RaM-ListNet Epoch 840/200 | Loss: 5.9556 | Pred Std: 0.8643
RaM-ListNet Epoch 860/200 | Loss: 5.8902 | Pred Std: 0.9517
RaM-ListNet Epoch 880/200 | Loss: 5.9160 | Pred Std: 0.8986
RaM-ListNet Epoch 900/200 | Loss: 5.8034 | Pred Std: 0.9735
RaM-ListNet Epoch 920/200 | Loss: 6.0368 | Pred Std: 0.9429
RaM-ListNet Epoch 940/200 | Loss: 5.9039 | Pred Std: 0.9008
RaM-ListNet Epoch 960/200 | Loss: 5.8400 | Pred Std: 0.8621
RaM-ListNet Epoch 980/200 | Loss: 5.7722 | Pred Std: 0.8585
RaM-ListNet Epoch 1000/200 | Loss: 6.0073 | Pred Std: 0.9171
RaM-ListNet Epoch 1020/200 | Loss: 5.8713 | Pred Std: 0.9070
RaM-ListNet Epoch 1040/200 | Loss: 5.7466 | Pred Std: 0.9941
RaM-ListNet Epoch 1060/200 | Loss: 5.9367 | Pred Std: 0.8702
RaM-ListNet Epoch 1080/200 | Loss: 5.7670 | Pred Std: 0.9178
RaM-ListNet Epoch 1100/200 | Loss: 5.6123 | Pred Std: 0.8983
RaM-ListNet Epoch 1120/200 | Loss: 5.6598 | Pred Std: 0.9606
RaM-ListNet Epoch 1140/200 | Loss: 5.8103 | Pred Std: 0.9130
RaM-ListNet Epoch 1160/200 | Loss: 5.8001 | Pred Std: 0.9324
RaM-ListNet Epoch 1180/200 | Loss: 6.1350 | Pred Std: 0.9093
RaM-ListNet Epoch 1200/200 | Loss: 5.8417 | Pred Std: 0.8380
RaM-ListNet Epoch 1220/200 | Loss: 6.0624 | Pred Std: 0.9342
RaM-ListNet Epoch 1240/200 | Loss: 6.0893 | Pred Std: 0.8640
RaM-ListNet Epoch 1260/200 | Loss: 5.6250 | Pred Std: 0.8768
RaM-ListNet Epoch 1280/200 | Loss: 5.6922 | Pred Std: 0.9237
RaM-ListNet Epoch 1300/200 | Loss: 5.8754 | Pred Std: 0.9174
RaM-ListNet Epoch 1320/200 | Loss: 5.5905 | Pred Std: 0.9783
RaM-ListNet Epoch 1340/200 | Loss: 5.7136 | Pred Std: 0.8991
RaM-ListNet Epoch 1360/200 | Loss: 5.9307 | Pred Std: 0.9330
RaM-ListNet Epoch 1380/200 | Loss: 5.6730 | Pred Std: 0.9396
RaM-ListNet Epoch 1400/200 | Loss: 5.6204 | Pred Std: 0.9505
RaM-ListNet Epoch 1420/200 | Loss: 5.7875 | Pred Std: 0.9486
RaM-ListNet Epoch 1440/200 | Loss: 5.7851 | Pred Std: 0.9342
RaM-ListNet Epoch 1460/200 | Loss: 5.6094 | Pred Std: 1.0058
RaM-ListNet Epoch 1480/200 | Loss: 5.8914 | Pred Std: 1.0364
RaM-ListNet Epoch 1500/200 | Loss: 5.6299 | Pred Std: 0.9428
RaM-ListNet Epoch 1520/200 | Loss: 5.8166 | Pred Std: 0.9339
RaM-ListNet Epoch 1540/200 | Loss: 5.9978 | Pred Std: 0.9964
RaM-ListNet Epoch 1560/200 | Loss: 5.6204 | Pred Std: 0.9344
RaM-ListNet Epoch 1580/200 | Loss: 5.7114 | Pred Std: 0.9351
RaM-ListNet Epoch 1600/200 | Loss: 5.8214 | Pred Std: 0.9165
RaM-ListNet Epoch 1620/200 | Loss: 5.5844 | Pred Std: 0.9645
RaM-ListNet Epoch 1640/200 | Loss: 5.6346 | Pred Std: 0.9472
RaM-ListNet Epoch 1660/200 | Loss: 5.7166 | Pred Std: 0.9230
RaM-ListNet Epoch 1680/200 | Loss: 5.7021 | Pred Std: 0.9650
RaM-ListNet Epoch 1700/200 | Loss: 5.4795 | Pred Std: 0.9747
RaM-ListNet Epoch 1720/200 | Loss: 5.9595 | Pred Std: 0.9161
RaM-ListNet Epoch 1740/200 | Loss: 5.7242 | Pred Std: 0.9876
RaM-ListNet Epoch 1760/200 | Loss: 5.8858 | Pred Std: 0.9961
RaM-ListNet Epoch 1780/200 | Loss: 5.5118 | Pred Std: 1.0186
RaM-ListNet Epoch 1800/200 | Loss: 5.6526 | Pred Std: 0.9944
RaM-ListNet Epoch 1820/200 | Loss: 5.9138 | Pred Std: 0.9726
RaM-ListNet Epoch 1840/200 | Loss: 5.9010 | Pred Std: 1.0074
RaM-ListNet Epoch 1860/200 | Loss: 5.8082 | Pred Std: 1.0624
RaM-ListNet Epoch 1880/200 | Loss: 5.4119 | Pred Std: 1.0482
RaM-ListNet Epoch 1900/200 | Loss: 5.8121 | Pred Std: 1.1210
RaM-ListNet Epoch 1920/200 | Loss: 5.7446 | Pred Std: 0.9634
RaM-ListNet Epoch 1940/200 | Loss: 5.9075 | Pred Std: 0.9578
RaM-ListNet Epoch 1960/200 | Loss: 5.8068 | Pred Std: 1.0546
RaM-ListNet Epoch 1980/200 | Loss: 5.5888 | Pred Std: 0.9536
RaM-ListNet Epoch 2000/200 | Loss: 5.7233 | Pred Std: 0.9793
Performing Output Adaptation...
Proxy Statistics: Mean=-1.0275, Std=1.0368
These will be used to normalize gradients during sampling.

Start Training Flow Model...
Flow Epoch 10/100 | Loss: 0.412155
Flow Epoch 20/100 | Loss: 0.306580
Flow Epoch 30/100 | Loss: 0.260393
Flow Epoch 40/100 | Loss: 0.237666
Flow Epoch 50/100 | Loss: 0.223086
Flow Epoch 60/100 | Loss: 0.215310
Flow Epoch 70/100 | Loss: 0.207949
Flow Epoch 80/100 | Loss: 0.202203
Flow Epoch 90/100 | Loss: 0.194114
Flow Epoch 100/100 | Loss: 0.191372
DEBUG: Top 5 indices: [17123 11687 28136 32111 15662]
DEBUG: Start Scores Mean: 0.3041 (Should be > 0.9)
normalized_scores: [0.2147157  0.5292747  0.67758626 0.36628458 0.91143364 0.26418382
 0.21447863 0.6497959  0.568507   0.601874   0.27192512 0.87806666
 0.58509254 0.40477467 0.63312787 0.5117511  0.4790129  0.3618934
 0.42355585 0.34900838 0.63194245 0.38288048 0.31056982 0.52261573
 0.21985939 0.7421659  0.5433967  0.56343544 0.7441966  0.6749887
 0.3276605  0.2263122  0.4790129  0.6227271  0.18065806 0.6285202
 0.4899394  0.4478209  0.2632458  0.2960974  0.28771698 0.5278316
 0.26502907 0.2319816  0.27308992 0.5601781  0.20445924 0.24608296
 0.28545952 0.37948912 0.6169134  0.8693976  0.8594916  0.4143405
 0.6285202  0.22313735 0.66696906 0.5237084  0.3961675  0.5875459
 0.64266276 0.47135407 0.44765595 0.14631179 0.7152826  0.21622068
 0.22313735 0.4192471  0.7761205  0.15251721 0.81360036 0.23252794
 0.87806666 0.4957325  0.10978024 0.31324992 0.8827155  0.6497959
 0.46723086 0.50239146 0.46513835 0.61404777 0.4899394  0.6368284
 0.8351544  0.39186904 0.7761205  0.48118788 0.1166866  0.5479219
 0.86301696 0.31199232 0.31289944 0.22313735 0.17729765 0.63075703
 0.42113346 0.43884262 0.2851709  0.5282542  0.43524513 0.56343544
 0.6746176  0.37839648 0.3151878  0.6901724  0.41606194 0.85094625
 0.45019174 0.38492146 0.5394178  0.21447863 0.4034037  0.5090092
 0.3446481  0.2739558  0.37741724 0.3630788  0.22313735 0.5223065
 0.273193   0.43027666 0.45952046 0.3887148  0.78098583 0.6739682
 0.34873006 0.73857874]
------------------------------
Optimization Results (Proxy Screened Top 128):
Optimized Score Mean (valid only): 0.4716
Optimized Score Max (valid only):  0.9114
Normalized Percentile Scores (100th / 80th / 50th): 0.9114 | 0.6403 | 0.4549
------------------------------
Seed: 7 completed
==============================
Average Normalized Percentile Scores across seeds (100th / 80th / 50th): 0.9528 | 0.6966 | 0.4991
Std of Normalized Percentile Scores across seeds (100th / 80th / 50th): 0.0309 | 0.0694 | 0.0461
==============================
