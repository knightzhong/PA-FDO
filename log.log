nohup: 忽略输入
Detected Discrete Task: TFBind8-Exact-v0. Applying One-Hot + Dequantization.
Data Loaded: Input Dim=32 | Total 32898 | Gold 3289

Training RankProxy with ListNet Loss (RaM Strategy)...
RaM-ListNet Epoch 20/2000 | Loss: 6.1816 | Pred Std: 0.6373
RaM-ListNet Epoch 40/2000 | Loss: 6.1755 | Pred Std: 0.5980
RaM-ListNet Epoch 60/2000 | Loss: 6.1944 | Pred Std: 0.5684
RaM-ListNet Epoch 80/2000 | Loss: 6.0300 | Pred Std: 0.5934
RaM-ListNet Epoch 100/2000 | Loss: 5.9339 | Pred Std: 0.6875
RaM-ListNet Epoch 120/2000 | Loss: 5.9024 | Pred Std: 0.7563
RaM-ListNet Epoch 140/2000 | Loss: 6.0598 | Pred Std: 0.7545
RaM-ListNet Epoch 160/2000 | Loss: 6.0349 | Pred Std: 0.6345
RaM-ListNet Epoch 180/2000 | Loss: 5.8782 | Pred Std: 0.7337
RaM-ListNet Epoch 200/2000 | Loss: 6.1177 | Pred Std: 0.7485
RaM-ListNet Epoch 220/2000 | Loss: 5.8930 | Pred Std: 0.7698
RaM-ListNet Epoch 240/2000 | Loss: 5.9338 | Pred Std: 0.7393
RaM-ListNet Epoch 260/2000 | Loss: 5.8904 | Pred Std: 0.7883
RaM-ListNet Epoch 280/2000 | Loss: 6.0421 | Pred Std: 0.7353
RaM-ListNet Epoch 300/2000 | Loss: 5.9043 | Pred Std: 0.8160
RaM-ListNet Epoch 320/2000 | Loss: 6.1264 | Pred Std: 0.8362
RaM-ListNet Epoch 340/2000 | Loss: 5.8371 | Pred Std: 0.7681
RaM-ListNet Epoch 360/2000 | Loss: 5.8989 | Pred Std: 0.7685
RaM-ListNet Epoch 380/2000 | Loss: 5.9955 | Pred Std: 0.8620
RaM-ListNet Epoch 400/2000 | Loss: 6.0759 | Pred Std: 0.8125
RaM-ListNet Epoch 420/2000 | Loss: 6.1026 | Pred Std: 0.7791
RaM-ListNet Epoch 440/2000 | Loss: 6.0795 | Pred Std: 0.8287
RaM-ListNet Epoch 460/2000 | Loss: 5.9603 | Pred Std: 0.8182
RaM-ListNet Epoch 480/2000 | Loss: 6.0390 | Pred Std: 0.8011
RaM-ListNet Epoch 500/2000 | Loss: 5.9581 | Pred Std: 0.7830
RaM-ListNet Epoch 520/2000 | Loss: 5.9439 | Pred Std: 0.7895
RaM-ListNet Epoch 540/2000 | Loss: 5.8832 | Pred Std: 0.8201
RaM-ListNet Epoch 560/2000 | Loss: 5.8852 | Pred Std: 0.8088
RaM-ListNet Epoch 580/2000 | Loss: 5.7051 | Pred Std: 0.8454
RaM-ListNet Epoch 600/2000 | Loss: 5.9163 | Pred Std: 0.8168
RaM-ListNet Epoch 620/2000 | Loss: 5.8661 | Pred Std: 0.8726
RaM-ListNet Epoch 640/2000 | Loss: 5.9566 | Pred Std: 0.8499
RaM-ListNet Epoch 660/2000 | Loss: 5.8462 | Pred Std: 0.9074
RaM-ListNet Epoch 680/2000 | Loss: 6.0222 | Pred Std: 0.8733
RaM-ListNet Epoch 700/2000 | Loss: 5.7288 | Pred Std: 0.7841
RaM-ListNet Epoch 720/2000 | Loss: 5.7857 | Pred Std: 0.8529
RaM-ListNet Epoch 740/2000 | Loss: 5.8355 | Pred Std: 0.8886
RaM-ListNet Epoch 760/2000 | Loss: 5.8916 | Pred Std: 0.9499
RaM-ListNet Epoch 780/2000 | Loss: 5.6904 | Pred Std: 0.8774
RaM-ListNet Epoch 800/2000 | Loss: 5.7807 | Pred Std: 0.9497
RaM-ListNet Epoch 820/2000 | Loss: 5.7672 | Pred Std: 0.8442
RaM-ListNet Epoch 840/2000 | Loss: 5.7509 | Pred Std: 0.8861
RaM-ListNet Epoch 860/2000 | Loss: 5.8764 | Pred Std: 0.9137
RaM-ListNet Epoch 880/2000 | Loss: 5.6093 | Pred Std: 0.9078
RaM-ListNet Epoch 900/2000 | Loss: 5.9191 | Pred Std: 0.8420
RaM-ListNet Epoch 920/2000 | Loss: 5.8472 | Pred Std: 0.8653
RaM-ListNet Epoch 940/2000 | Loss: 6.0540 | Pred Std: 0.8804
RaM-ListNet Epoch 960/2000 | Loss: 5.6989 | Pred Std: 0.9491
RaM-ListNet Epoch 980/2000 | Loss: 5.9571 | Pred Std: 0.9287
RaM-ListNet Epoch 1000/2000 | Loss: 5.8374 | Pred Std: 0.8850
RaM-ListNet Epoch 1020/2000 | Loss: 5.8464 | Pred Std: 0.9285
RaM-ListNet Epoch 1040/2000 | Loss: 5.7314 | Pred Std: 0.9917
RaM-ListNet Epoch 1060/2000 | Loss: 5.6151 | Pred Std: 0.9472
RaM-ListNet Epoch 1080/2000 | Loss: 5.5949 | Pred Std: 0.8874
RaM-ListNet Epoch 1100/2000 | Loss: 5.9623 | Pred Std: 0.9147
RaM-ListNet Epoch 1120/2000 | Loss: 5.7621 | Pred Std: 0.8713
RaM-ListNet Epoch 1140/2000 | Loss: 5.5771 | Pred Std: 0.9272
RaM-ListNet Epoch 1160/2000 | Loss: 5.8692 | Pred Std: 0.9668
RaM-ListNet Epoch 1180/2000 | Loss: 5.8307 | Pred Std: 0.9493
RaM-ListNet Epoch 1200/2000 | Loss: 5.8568 | Pred Std: 0.9026
RaM-ListNet Epoch 1220/2000 | Loss: 5.6895 | Pred Std: 0.9123
RaM-ListNet Epoch 1240/2000 | Loss: 5.8118 | Pred Std: 0.9769
RaM-ListNet Epoch 1260/2000 | Loss: 5.6470 | Pred Std: 0.8895
RaM-ListNet Epoch 1280/2000 | Loss: 5.7854 | Pred Std: 1.0076
RaM-ListNet Epoch 1300/2000 | Loss: 5.9144 | Pred Std: 0.9167
RaM-ListNet Epoch 1320/2000 | Loss: 5.9924 | Pred Std: 0.9020
RaM-ListNet Epoch 1340/2000 | Loss: 5.8598 | Pred Std: 1.0289
RaM-ListNet Epoch 1360/2000 | Loss: 5.6969 | Pred Std: 0.9498
RaM-ListNet Epoch 1380/2000 | Loss: 5.6840 | Pred Std: 0.9719
RaM-ListNet Epoch 1400/2000 | Loss: 5.7879 | Pred Std: 0.9107
RaM-ListNet Epoch 1420/2000 | Loss: 5.6858 | Pred Std: 1.0321
RaM-ListNet Epoch 1440/2000 | Loss: 5.7508 | Pred Std: 0.9154
RaM-ListNet Epoch 1460/2000 | Loss: 5.7579 | Pred Std: 0.9360
RaM-ListNet Epoch 1480/2000 | Loss: 5.8025 | Pred Std: 0.8999
RaM-ListNet Epoch 1500/2000 | Loss: 5.7584 | Pred Std: 1.0317
RaM-ListNet Epoch 1520/2000 | Loss: 6.0796 | Pred Std: 0.9828
RaM-ListNet Epoch 1540/2000 | Loss: 5.5229 | Pred Std: 0.9780
RaM-ListNet Epoch 1560/2000 | Loss: 5.5998 | Pred Std: 1.0555
RaM-ListNet Epoch 1580/2000 | Loss: 5.7892 | Pred Std: 0.9383
RaM-ListNet Epoch 1600/2000 | Loss: 5.7635 | Pred Std: 0.9358
RaM-ListNet Epoch 1620/2000 | Loss: 5.6067 | Pred Std: 1.0290
RaM-ListNet Epoch 1640/2000 | Loss: 5.6671 | Pred Std: 0.9833
RaM-ListNet Epoch 1660/2000 | Loss: 5.6431 | Pred Std: 1.1026
RaM-ListNet Epoch 1680/2000 | Loss: 5.8139 | Pred Std: 0.9145
RaM-ListNet Epoch 1700/2000 | Loss: 5.7140 | Pred Std: 1.0770
RaM-ListNet Epoch 1720/2000 | Loss: 5.7029 | Pred Std: 0.9831
RaM-ListNet Epoch 1740/2000 | Loss: 5.8057 | Pred Std: 1.0002
RaM-ListNet Epoch 1760/2000 | Loss: 5.5140 | Pred Std: 0.9803
RaM-ListNet Epoch 1780/2000 | Loss: 5.8836 | Pred Std: 1.0108
RaM-ListNet Epoch 1800/2000 | Loss: 5.7429 | Pred Std: 0.9683
RaM-ListNet Epoch 1820/2000 | Loss: 5.7447 | Pred Std: 0.9588
RaM-ListNet Epoch 1840/2000 | Loss: 5.5775 | Pred Std: 1.0299
RaM-ListNet Epoch 1860/2000 | Loss: 5.7426 | Pred Std: 1.0864
RaM-ListNet Epoch 1880/2000 | Loss: 5.7942 | Pred Std: 0.9238
RaM-ListNet Epoch 1900/2000 | Loss: 5.7520 | Pred Std: 1.0184
RaM-ListNet Epoch 1920/2000 | Loss: 5.5741 | Pred Std: 0.9773
RaM-ListNet Epoch 1940/2000 | Loss: 5.6177 | Pred Std: 0.9986
RaM-ListNet Epoch 1960/2000 | Loss: 5.7146 | Pred Std: 0.9610
RaM-ListNet Epoch 1980/2000 | Loss: 5.5339 | Pred Std: 1.0675
RaM-ListNet Epoch 2000/2000 | Loss: 5.6194 | Pred Std: 0.9888

Training Flow Model (PA-FDO Dynamic)...

[Check Training Data] y_anc mean: 0.0384 | min: -3.4211 | max: 1.4793
[Train Debug] v_pred norm: 1.07 | Target u_t norm: 4.82
[Train Debug] v_pred norm: 1.00 | Target u_t norm: 4.66
Step 500/5000 | Loss: 0.6802
[Train Debug] v_pred norm: 1.19 | Target u_t norm: 4.80
[Train Debug] v_pred norm: 1.30 | Target u_t norm: 4.87
Step 1000/5000 | Loss: 0.6638
[Train Debug] v_pred norm: 1.41 | Target u_t norm: 4.73
[Train Debug] v_pred norm: 1.43 | Target u_t norm: 4.84
[Train Debug] v_pred norm: 1.52 | Target u_t norm: 4.80
[Train Debug] v_pred norm: 1.61 | Target u_t norm: 4.88
Step 1500/5000 | Loss: 0.6464
[Train Debug] v_pred norm: 1.65 | Target u_t norm: 4.72
[Train Debug] v_pred norm: 1.67 | Target u_t norm: 4.78
[Train Debug] v_pred norm: 1.67 | Target u_t norm: 4.74
[Train Debug] v_pred norm: 1.63 | Target u_t norm: 4.79
[Train Debug] v_pred norm: 1.68 | Target u_t norm: 4.83
Step 2000/5000 | Loss: 0.6251
[Train Debug] v_pred norm: 1.75 | Target u_t norm: 4.86
[Train Debug] v_pred norm: 1.83 | Target u_t norm: 4.71
Step 2500/5000 | Loss: 0.6260
[Train Debug] v_pred norm: 1.92 | Target u_t norm: 4.75
[Train Debug] v_pred norm: 1.96 | Target u_t norm: 4.76
[Train Debug] v_pred norm: 1.88 | Target u_t norm: 4.77
[Train Debug] v_pred norm: 1.82 | Target u_t norm: 4.71
Step 3000/5000 | Loss: 0.6245
[Train Debug] v_pred norm: 1.96 | Target u_t norm: 4.75
[Train Debug] v_pred norm: 1.95 | Target u_t norm: 4.67
[Train Debug] v_pred norm: 2.16 | Target u_t norm: 4.76
[Train Debug] v_pred norm: 1.94 | Target u_t norm: 4.77
[Train Debug] v_pred norm: 2.05 | Target u_t norm: 4.84
[Train Debug] v_pred norm: 2.11 | Target u_t norm: 4.79
[Train Debug] v_pred norm: 1.92 | Target u_t norm: 4.72
[Train Debug] v_pred norm: 2.08 | Target u_t norm: 4.77
Step 3500/5000 | Loss: 0.6042
[Train Debug] v_pred norm: 2.17 | Target u_t norm: 4.78
[Train Debug] v_pred norm: 1.99 | Target u_t norm: 4.73
[Train Debug] v_pred norm: 2.21 | Target u_t norm: 4.74
[Train Debug] v_pred norm: 2.28 | Target u_t norm: 4.72
Step 4000/5000 | Loss: 0.5897
[Train Debug] v_pred norm: 2.34 | Target u_t norm: 4.71
[Train Debug] v_pred norm: 2.09 | Target u_t norm: 4.70
[Train Debug] v_pred norm: 2.10 | Target u_t norm: 4.76
Step 4500/5000 | Loss: 0.5362
[Train Debug] v_pred norm: 2.19 | Target u_t norm: 4.78
[Train Debug] v_pred norm: 2.26 | Target u_t norm: 4.77
[Train Debug] v_pred norm: 2.54 | Target u_t norm: 4.81
[Train Debug] v_pred norm: 2.26 | Target u_t norm: 4.78
[Train Debug] v_pred norm: 2.30 | Target u_t norm: 4.84
Step 5000/5000 | Loss: 0.5380

Running Evaluation with Energy-based Guidance...
[Info] Safe Targets: Mean=1.4803 | Max=1.4803

[Debug] Start Sampling. X Range: [-0.97, 2.30]
[Debug] Y_Target Stats: Mean=1.4803 | Min=1.4803 | Max=1.4803
[Debug] Y_Start  Stats: Mean=0.6720  | Min=0.1473 | Max=1.2010
[Step 0] X_max: 2.30 | Flow: 15.50 | Grad: 9.15 | Reg: 3.22
[Step 0] ... Total(Clipped): 20.45
[Step 10] X_max: 2.28 | Flow: 22.13 | Grad: 8.31 | Reg: 3.15
[Step 10] ... Total(Clipped): 22.34
[Step 20] X_max: 2.28 | Flow: 28.44 | Grad: 7.40 | Reg: 3.09
[Step 20] ... Total(Clipped): 22.30
[Step 30] X_max: 2.27 | Flow: 30.22 | Grad: 6.60 | Reg: 3.05
[Step 30] ... Total(Clipped): 20.87
[Step 40] X_max: 2.30 | Flow: 32.45 | Grad: 5.78 | Reg: 3.05
[Step 40] ... Total(Clipped): 18.46
[Step 50] X_max: 2.37 | Flow: 39.27 | Grad: 4.71 | Reg: 3.06
[Step 50] ... Total(Clipped): 18.39
[Step 60] X_max: 2.44 | Flow: 51.76 | Grad: 3.71 | Reg: 3.08
[Step 60] ... Total(Clipped): 19.49
[Step 70] X_max: 2.51 | Flow: 69.58 | Grad: 2.86 | Reg: 3.11
[Step 70] ... Total(Clipped): 21.13
[Step 80] X_max: 2.59 | Flow: 80.22 | Grad: 1.78 | Reg: 3.16
[Step 80] ... Total(Clipped): 22.52
[Step 90] X_max: 2.67 | Flow: 74.20 | Grad: 0.90 | Reg: 3.22
[Step 90] ... Total(Clipped): 22.61
tensor([[ 0.0018,  0.0629,  1.1736,  ...,  0.1210,  1.1583,  0.1370],
        [-0.1744,  0.7589, -0.1187,  ..., -0.1230, -0.1277,  0.9928],
        [-0.1424, -0.1501,  0.8763,  ..., -0.0339,  0.0187,  1.0356],
        ...,
        [ 0.7087, -0.1269, -0.3702,  ..., -0.0570, -0.0218,  0.9636],
        [ 0.0075, -0.0876,  0.0204,  ..., -0.0926,  1.0372, -0.0034],
        [ 0.7854, -0.2066, -0.0755,  ..., -0.0877, -0.0783,  0.8117]])
[0.4055787  0.3795819  0.41208303 0.3959201  0.36774832 0.38904464
 0.39185873 0.39111656 0.3299695  0.3683668  0.34329775 0.37438667
 0.3767575  0.37190244 0.39354926 0.34716323 0.33892715 0.38440606
 0.40360987 0.35223478 0.385107   0.3692739  0.34749308 0.37668535
 0.3739125  0.35257494 0.35465714 0.3686554  0.35037935 0.40140396
 0.34690553 0.39021978 0.3889725  0.34357604 0.37511855 0.37940666
 0.40852678 0.38132396 0.36155322 0.35376036 0.38255063 0.36674845
 0.38091165 0.3816126  0.36276954 0.39998144 0.41462913 0.35633737
 0.38569456 0.40030098 0.34752402 0.38393188 0.3555849  0.37847894
 0.35657445 0.35604873 0.39266276 0.4042799  0.4101142  0.4120212
 0.40940297 0.37754092 0.38762215 0.33739126 0.38151982 0.34290603
 0.41376325 0.38593164 0.40144518 0.3406795  0.40580547 0.37459284
 0.40554777 0.38078794 0.359739   0.3519977  0.37810788 0.3781594
 0.37332496 0.36714014 0.3418134  0.36482084 0.37548962 0.4179483
 0.362182   0.3944151  0.4071558  0.36950067 0.4102173  0.3535336
 0.3466994  0.37362388 0.36495486 0.37998393 0.38325155 0.3425968
 0.34052488 0.3860038  0.41425803 0.36370757 0.38413805 0.40742382
 0.39571393 0.35247186 0.41792768 0.34150416 0.35656413 0.3639859
 0.33942193 0.3454418  0.3530388  0.38546777 0.38299385 0.41392818
 0.34418422 0.37168598 0.40241414 0.4113821  0.38249907 0.40901124
 0.40721765 0.40450665 0.39685813 0.35075042 0.4022286  0.38172597
 0.39681688 0.37381974]
------------------------------
Result (Valid 128): Mean 0.3772
Percentiles (100/80/50): 0.4179 | 0.4014 | 0.3783
------------------------------
